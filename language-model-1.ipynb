{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1IXTKYBVLYiB"
   },
   "source": [
    "# Assignment #4\n",
    "**CE4719: Deep Learing**\n",
    "\n",
    "\n",
    "*   Spring 2020\n",
    "*   http://ce.sharif.edu/courses/98-99/2/ce719-1/index.php\n",
    "\n",
    "**Please pay attention to these notes:**\n",
    "\n",
    "<br/>\n",
    "\n",
    "- The items you need to answer are highlighted in red and the coding parts you need to implement are denoted by:\n",
    "```\n",
    "      ########################################\n",
    "      #     Put your implementation here     #\n",
    "      ########################################\n",
    "```\n",
    "- We always recommend discussion in groups for assignments. However, each student should finish all of the questions by him/herself. \n",
    "- All submitted code will be compared against all student's codes using Stanford MOSS.\n",
    "- If you have any questions about this assignment, feel free to drop us a line. You may also post your questions on the course's forum page.\n",
    "- We HIGHLY encourage you to run this notebook on Google Colab.\n",
    "- **Before starting to work on the assignment, please fill your name in the next section *AND remember to RUN the cell.***\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "9XcXOHW3sLl0"
   },
   "outputs": [],
   "source": [
    "#@title Enter your information & \"RUN the cell!!\"\n",
    "student_id = \"98208824\" #@param {type:\"string\"}\n",
    "student_name = \"Sadroddin Barikbin\" #@param {type:\"string\"}\n",
    "\n",
    "print(\"your student id:\", student_id)\n",
    "print(\"your name:\", student_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TtknBbi1hnTE"
   },
   "source": [
    "## 1. Language Model (29 + 7 pts)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2NrRTj1cMHlK"
   },
   "source": [
    "\n",
    "Recurrent neural networks have shown excellent performance in language modeling, especially in recognizing a language's underlying structure and grammar. Having a well-trained language model can help us to understand natural language inputs, which can improve down-stream tasks. However, training a recurrent neural network has always been a challenging task. In this section, we will train a neural language model and get familiar with regularization techniques that are specially optimized for RNNs.\n",
    "\n",
    "For this section, we will use [Wikitext](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) as our dataset, which is a corpus collected from Wikipedia's featured articles. Wikitext has two distinct features: 1- It is a document-level dataset and will enable the model to learn long-term dependencies (generating a paragraph) 2- It is a well-cleaned dataset and can be used with minimum pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NsUUzKSCha5f"
   },
   "source": [
    "### 1.1 Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kxBENFBuhp66"
   },
   "source": [
    "First, let's download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14814,
     "status": "ok",
     "timestamp": 1588448614729,
     "user": {
      "displayName": "sadra barikbein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCq0YqNHWfcZyly3I_AMTkvS8_tO8PZOQpGtkK=s64",
      "userId": "07513140462530453530"
     },
     "user_tz": -270
    },
    "id": "JqkuGgaGhfBf",
    "outputId": "c65e5a11-acab-4b73-b4e2-8c7ab385b4bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  wikitext-2-v2.zip\n",
      "  inflating: wikitext-2/train.txt    \n",
      "  inflating: wikitext-2/valid.txt    \n",
      "  inflating: wikitext-2/vocab.pk     \n"
     ]
    }
   ],
   "source": [
    "! wget -q https://github.com/kazemnejad/ce4719-hw04-assets/raw/master/wikitext-2-v2.zip\n",
    "! unzip -e wikitext-2-v2.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vrJ_Jueb4v3u"
   },
   "source": [
    "Here is a sneak peek at our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7930,
     "status": "ok",
     "timestamp": 1588259236275,
     "user": {
      "displayName": "sadra barikbein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCq0YqNHWfcZyly3I_AMTkvS8_tO8PZOQpGtkK=s64",
      "userId": "07513140462530453530"
     },
     "user_tz": -270
    },
    "id": "mErcBa6q64Ji",
    "outputId": "356a496a-0930-4b8a-938b-c792dc867b0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = Berkley Bedell = <eol> <eol> Berkley Warren Bedell ( born March 5 , 1921 ) is a former U.S. Representative from Iowa . After starting a successful business in his youth , Berkley Fly Co . , he ran for the United States Congress in 1972 , but was defeated by incumbent Wiley Mayne . In 1974 , however , Bedell beat Wiley Mayne and was elected to Congress . <eol> He was known for his support of representative democracy and his populist style . For example , he would hold town halls and let constituents vote on motions to decide what he would do in Congress on their behalf . These meetings helped Bedell understand the problems of his constituents ; as a result , he backed issues that were important to his farming constituency , such as waterway usage fees and production constraints . <eol> He did not seek reelection in 1986 after contracting <unk> disease from a tick bite . Though he no longer serves in Congress , Bedell remains active in Iowa politics , strongly supporting Howard Dean in 2004 over John Kerry . In the 2008 presidential election , he met several times with Chris Dodd , but endorsed Barack Obama in the end . <eol> <eol> = = Early life = = <eol> <eol> Born in Spirit Lake , Iowa , Bedell was educated in Spirit Lake public schools . He graduated from Spirit Lake High School in 1939 , where he earned spending money with a business in the midst of the Great Depression . His business involved <unk> dog hairs around <unk> , the result of which could be sold as trout flies . He began tying the fly @-@ fishing <unk> in his bedroom , then he moved the business into his parents ' basement . In time , he got space above a grocery store to continue the business full @-@ time . <eol> After graduating from high school , he attended Iowa State University from 1940 to 1942 , where he met fellow <unk> student Elinor <unk> from Grand <unk> , Minnesota . Berkley and Elinor married in Minneapolis on August 29 , 1943 and their son Kenneth was born in 1947 , Thomas in 1950 and daughter <unk> in 1952 . Berkley ’ s college and personal life was interrupted in 1942 when he joined the army . He served in the United States Army as first lieutenant and flight trainer from 1942 to 1945 . When he got back , he began to garner success from his fish tackling business . His business became larger , with hundreds of employees and international operations ; he had become a millionaire by the 1960s . He served as member of the Spirit Lake Board of Education from 1957 to 1962 . <eol> <eol> = = Political career = = <eol> <eol> <eol> = = = Running for Congress = = = <eol> <eol> By the early 1970s , Bedell had decided to run for political office . In 1972 , he ran against Wiley Mayne , a Republican incumbent in Iowa 's 6th congressional district . Mayne was a staunch supporter of Richard Nixon and secured victory along with the President in a year favorable to the Republicans . Mayne , however , would politically suffer after Watergate ( he was one of only a few Republicans to vote against <unk> the President on the judiciary committee . ) The damage had already been done , and Bedell defeated Mayne in a 1974 rematch . <eol> During his time at Congress , Bedell took efforts to uphold representative democracy . He held town halls regularly with his constituents , and he would let them vote on motions to decide what he would do in Congress on their behalf . This type of communication told Bedell of the types of issues affecting his farming constituency . Thus , though Bedell had not farmed in his life , he would take steps in Congress to benefit farmers . <eol> <eol> = = = Waterway usage fees = = = <eol> <eol> Bedell sponsored several bold initiatives during his tenure in the United States House of Representatives . One initiative , which came from his constituents ' problems with the barge industry , focused on waterway usage fees . He introduced legislation in 1977 that would require the barge industry to pay a fee for using the waterways which , Bedell pointed out , the Government paid millions of dollars to create and maintain . Bedell 's original plan set the rate the barge industry paid as directly related to the amount the Government spent on waterway projects . This would have the additional effect of helping curb unnecessary waterway projects , and it was the same plan proposed by Pete Domenici in the Senate . <eol> Congress eventually passed a watered @-@ down version of the original plan put forward by Bedell and Senator Pete Domenici . The compromise version enacted a tax on the gasoline barges used and put it into a \" trust \" for waterway projects . While other supporters of waterway usage fees , including Domenici , backed the compromise , Bedell gave a <unk> plea for his colleagues to oppose it . He viewed it as lacking a crucial element of the original plan - that of capital recovery . The trust was optional , and the Government could spend money on waterway projects irrespective of the trust . The compromise was eventually signed by Jimmy Carter . Bedell 's original plan never made it through the House of Representatives , but he continued to introduce it in succeeding sessions . It would not , however , get a floor vote in succeeding sessions . <eol> <eol> = = = <unk> issues = = = <eol> <eol> In 1985 , Bedell put forward an agricultural plan that he thought would increase production controls for farmers , thus raising prices for crops . This plan , backed by labor unions and certain Democrats , passed the Agriculture Committee as an amendment to farm legislation . It mandated a referendum that would then be used to determine what types of production controls to enact . The purpose of this plan was <unk> : production controls would decrease the aggregate supply of crops , thus making individual crops cost more ( which would benefit farmers , who were in the middle of an acute debt crisis . ) Second , by styling it as a referendum , the farmers would get to decide the severity of the controls . <eol> On the other hand , opponents of the Bedell plan had a very different view of this legislation . Representatives such as Pat Roberts claimed that the referendum was redundant because the farmers already voted the politicians into office , and this bill was an example of the politicians not doing their jobs . The Reagan Administration opposed the bill because of their opposition to production controls , and the President threatened to veto the farm bill if Bedell 's plan was left in place . When the bill got to the floor , an amendment was proposed to strike this provision , and it was passed 251 @-@ 174 . <eol> <eol> = = = Investigations of large businesses = = = <eol> <eol> While in Congress , Berkley Bedell was Chairman of the Small Business <unk> , and he used this position to investigate <unk> on the part of large oil companies . He also claimed that certain large oil companies <unk> their \" <unk> taxes \" in certain cases and wanted to pass legislation to increase regulations on these corporations . <eol> In these investigations , Bedell quickly gained the support of small gasoline <unk> and Congressman Bill Nelson . The chief target , <unk> , was accused of not paying all of its taxes on <unk> crude oil . In the end , the government tried to make a case against <unk> , but it was eventually dropped in 1985 . Bedell used this opportunity to attack the Administration for \" not caring \" about small business owners , and he advocated that <unk> agencies put aside 1 @-@ 3 % of their research and development money for small businesses . <eol> <eol> = = = Clash with Reagan = = = <eol> <eol> In late 1982 , Congress passed a law which forbade the United States from funding groups aiming to overthrow the <unk> government of Nicaragua . Then , in 1983 , Bedell visited Nicaragua and Honduras along with Representative Robert G. <unk> . During the trip , Bedell spoke with soldiers , generals , governmental officials and members of the contras . His conclusion at the end of the trip was that Ronald Reagan was aiding the contras in violation of federal law . He promised to hold hearings after returning to Congress . Bedell would later join other House Democrats in demanding documents from the White House related to the contras , but the Reagan Administration refused to provide them . Bedell became <unk> with the Reagan Administration as the decade wore on . He called his Central American policies \" sheer <unk> , \" saying that the mining of <unk> was an acts of war . Bedell would retire from Congress before Reagan 's acts in Central America would <unk> with the Iran @-@ <unk> Affair . <eol> Furthermore , Bedell was a sharp critic of Reagan 's agricultural policies , calling for John Block to resign after calling his agricultural plan a failure that was \" dead on arrival \" in both the House and the Senate . Reagan 's agricultural plan consisted primarily of a gradual reduction in farm subsidies . He also attacked the Department of Agriculture for \" looking backward \" when it dismissed the only expert on organic farming . Also , as chairman of the <unk> on Department Operations , Research and Foreign Agriculture , which was in charge of regulating USDA operations , he opposed the proposals Reagan had for reforming the organization . The proposals generally involved shifting costs for meat inspections and other USDA duties from the federal government to the industry . <eol> <eol> = = = Controversy = = = <eol> <eol> In 1981 , it was revealed in internal <unk> that Bedell may have known about potential customs violations that his company engaged in . It asserted that Bedell had gone to Taiwan in 1973 to discuss \" prior violations of customs law \" in regards to the sale of fishing rods from the company 's Taiwan subsidiary . Bedell responded by denying any wrongdoing , saying that he has not been personally involved in the company in years . In the end , no charges were <unk> against him , and he was reelected after the story was published . <eol> <eol> = = After politics = = <eol> <eol> Bedell decided not to seek reelection in 1986 after contracting <unk> Disease from a tick bite . Since then , he has founded a center for alternative medicine and is a noted advocate of health freedom . Due largely to his friendship with Tom <unk> , he remains an important political figure in Iowa , with politicians such as Howard Dean meeting him in their trips to the state . Also , the Elinor Bedell State Park was established in 1998 on land donated by Berkley Bedell . The park is named after the Congressman 's wife . <eol> As an opponent of the Vietnam War , Bedell signed a petition urging against United States military intervention in Iraq . This petition was signed with the names of 70 former <unk> from the 1970s and was presented in a press conference on March 15 , 2003 . Bedell said that it was unbelievable for the United States to settle disputes with war , and he said that an Iraq war would be similar to the Vietnam War . <eol> In the 2004 presidential election , Bedell attacked John Kerry for voting for <unk> <unk> 's Freedom to Farm Act , which Bedell claims wrecked the farm program . Bedell would later officially endorse Howard Dean 's candidacy . For the 2008 election , Bedell met with Chris Dodd . However , in December 2007 , he announced his endorsement of Barack Obama . <eol> <eoa> \n",
      " = The Boat Race 1900 = <eol> <eol> The 57th Boat Race took place on 31 March 1900 . Held annually , the Boat Race is a side @-@ by @-@ side rowing race between crews from the Universities of Oxford and Cambridge along the River Thames . Cambridge won by twenty lengths in a record @-@ equalling time of 18 minutes 45 seconds , taking the overall record in the event to 32 – 24 in Oxford 's favour . <eol> <eol> = = Background = = <eol> <eol> The Boat Race is a side @-@ by @-@ side rowing competition between the University of Oxford ( sometimes referred to as the \" Dark Blues \" ) and the University of Cambridge ( sometimes referred to as the \" Light Blues \" ) . The race was first held in 1829 , and since 1845 has taken place on the 4 @.@ 2 @-@ mile ( 6 @.@ 8 km ) Championship Course on the River Thames in southwest London . The rivalry is a major point of honour between the two universities and followed throughout the United Kingdom and worldwide . Cambridge went into the race as reigning champions , having won the 1899 race by three @-@ and @-@ a @-@ quarter lengths , while Oxford led overall with 32 victories to Cambridge 's 23 ( excluding the \" dead heat \" of 1877 ) . Leading up to the race , Oxford suffered a variety of misfortune : M. C. <unk> was ordered by his doctor not to row , H. J. Hale was injured and president Felix <unk> contracted scarlet fever . <eol> Cambridge were coached by James <unk> Close , who had rowed for the Light Blues three times between 1872 and 1874 , and Stanley <unk> , five @-@ time Blue for Cambridge between 1886 and 1890 . Oxford 's coaches were Harcourt Gilbey Gold ( Dark Blue president the previous year and four @-@ time Blue ) and Douglas McLean ( an Oxford Blue five times between 1883 and 1887 ) . The umpire for the race for the eleventh year in a row was Frank <unk> who had won the event four consecutive times , rowing for Oxford in the 1866 , 1867 , 1868 and 1869 races . <eol> <eol> = = Crews = = <eol> <eol> The Cambridge crew weighed an average of 12 <unk> 4 @.@ 625 lb ( 78 @.@ 1 kg ) , 0 @.@ 25 pounds ( 0 @.@ 1 kg ) per rower more than their opponents . Oxford 's crew contained three members with Boat Race experience : C. E. Johnston , C. W. <unk> and <unk> G. S. Maclagan . Cambridge saw six of their 1899 crew return , including William Dudley Ward and Raymond <unk> <unk> @-@ Smith , both of whom were rowing in their third race . Eight of the nine Light Blues were students at Trinity College . Oxford 's stroke H. H. <unk> , a native of South Australia , was the only non @-@ British participant registered in the race . Author and former Oxford rower George Drinkwater suggested that this year 's Cambridge crew , along with the Oxford crew which rowed in the 1897 race , \" stand in a class by themselves among University crews . \" He also described the Oxford crew as \" one of the poorest that ever came from the Isis \" . <eol> <eol> = = Race = = <eol> <eol> Oxford won the toss and elected to start from the Surrey station , handing the Middlesex side of the river to Cambridge . In good conditions , umpire <unk> got the race under way at 2 : 00 p.m. whereupon Cambridge took the lead immediately . By <unk> Steps they were three lengths ahead and continued to draw away from the Dark Blues , to win by 20 lengths in a time of 18 minutes 45 seconds . It was the fastest winning time in the history of the event , equalling that set by Oxford in the 1893 race . Although it was the Light Blues ' second consecutive victory , it followed a run of nine consecutive wins for Oxford – overall the Dark Blues led 32 – 24 . <eol> <eoa> \n",
      " = Isabella Beeton = <eol> <eol> Isabella Mary Beeton ( née Mayson ; 14 March 1836 – 6 February 1865 ) , also known as Mrs Beeton , was an English journalist , editor and writer . Her name is particularly associated with her first book , the 1861 work Mrs Beeton 's Book of Household Management . She was born in London and , after schooling in Islington , north London , and Heidelberg , Germany , she married Samuel Orchart Beeton , an ambitious publisher and magazine editor . <eol> In 1857 , less than a year after the wedding , Isabella began writing for one of her husband 's publications , The Englishwoman 's Domestic Magazine . She translated French fiction and wrote the cookery column , though all the recipes were <unk> from other works or sent in by the magazine 's readers . In 1859 the Beetons launched a series of 48 @-@ page monthly supplements to The Englishwoman 's Domestic Magazine ; the 24 instalments were published in one volume as Mrs Beeton 's Book of Household Management in October 1861 , which sold 60 @,@ 000 copies in the first year . Isabella was working on an abridged version of her book , which was to be titled The Dictionary of Every @-@ Day Cookery , when she died of <unk> fever in February 1865 at the age of 28 . She gave birth to four children , two of whom died in infancy , and had several miscarriages . Two of her biographers , Nancy Spain and <unk> Hughes , <unk> the theory that Samuel had unknowingly contracted syphilis in a premarital liaison with a prostitute , and had unwittingly passed the disease on to his wife . <eol> The Book of Household Management has been edited , revised and enlarged several times since Isabella 's death and is still in print as at 2016 . Food writers have stated that the subsequent editions of the work were far removed from and inferior to the original version . Several cookery writers , including Elizabeth David and Clarissa Dickson Wright , have criticised Isabella 's work , particularly her use of other people 's recipes . Others , such as the food writer Bee Wilson , consider the <unk> <unk> , and that Beeton and her work should be thought extraordinary and admirable . Her name has become associated with knowledge and authority on Victorian cooking and home management , and the Oxford English Dictionary states that by 1891 the term Mrs Beeton had become used as a generic name for a domestic authority . She is also considered a strong influence in the building or shaping of a middle @-@ class identity of the Victorian era . <eol> <eol> = = Biography = = <eol> <eol> <eol> = = = Early life , 1836 – 54 = = = <eol> <eol> Isabella Mayson was born on 14 March 1836 in Marylebone , London . She was the eldest of three daughters to Benjamin Mayson , a linen factor ( merchant ) and his wife Elizabeth ( née <unk> ) . Shortly after Isabella 's birth the family moved to Milk Street , <unk> , from where Benjamin traded . He died when Isabella was four years old , and Elizabeth , pregnant and unable to cope with raising the children on her own while maintaining Benjamin 's business , sent her two elder daughters to live with relatives . Isabella went to live with her recently widowed paternal grandfather in Great <unk> , <unk> , though she was back with her mother within the next two years . <eol> Three years after Benjamin 's death Elizabeth married Henry <unk> , a widower with four children . Henry was the <unk> of Epsom Racecourse , and had been granted residence within the <unk> grounds . The family , including Elizabeth 's mother , moved to Surrey and over the next twenty years Henry and Elizabeth had a further thirteen children . Isabella was instrumental in her siblings ' upbringing , and collectively referred to them as a \" living cargo of children \" . The experience gave her much insight and experience in how to manage a family and its household . <eol> After a brief education at a boarding school in Islington , in 1851 Isabella was sent to school in Heidelberg , Germany , accompanied by her <unk> Jane <unk> . Isabella became proficient in the piano and excelled in French and German ; she also gained knowledge and experience in making pastry . She had returned to Epsom by the summer of 1854 and took further lessons in pastry @-@ making from a local baker . <eol> <eol> = = = Marriage and career , 1854 – 61 = = = <eol> <eol> Around 1854 Isabella began a relationship with Samuel Orchart Beeton . His family had lived in Milk Street at the same time as the <unk> — Samuel 's father still ran the <unk> Tavern there — and Samuel 's sisters had also attended the same Heidelberg school as Isabella . Samuel was the first British publisher of Harriet <unk> <unk> 's Uncle Tom 's Cabin in 1852 and had also released two innovative and pioneering journals : The Englishwoman 's Domestic Magazine in 1852 and the Boys ' Own magazine in 1855 . The couple entered into extensive correspondence in 1855 — in which Isabella signed her letters as \" <unk> \" — and they announced their engagement in June 1855 . The marriage took place at St Martin 's Church , Epsom , in July the following year , and was announced in The Times . Samuel was \" a discreet but firm believer in the equality of women \" and their relationship , both personal and professional , was an equal partnership . The couple went to Paris for a three @-@ week honeymoon , after which Samuel 's mother joined them in a visit to Heidelberg . They returned to Britain in August , when the <unk> moved into 2 Chandos <unk> , a large Italianate house in Pinner . <eol> Within a month of returning from their honeymoon Isabella was pregnant . A few weeks before the birth , Samuel persuaded his wife to contribute to The Englishwoman 's Domestic Magazine , a publication that the food writers Mary <unk> and Olive <unk> consider was \" designed to make women content with their lot inside the home , not to interest them in the world outside \" . The magazine was affordable , aimed at young middle class women and was commercially successful , selling 50 @,@ 000 issues a month by 1856 . Isabella began by translating French fiction for publication as stories or serials . Shortly afterwards she started to work on the cookery column — which had been <unk> for the previous six months following the departure of the previous correspondent — and the household article . The Beetons ' son , Samuel Orchart , was born towards the end of May 1857 , but died at the end of August that year . On the death certificate , the cause of death was given as <unk> and cholera , although Hughes <unk> that Samuel senior had unknowingly contracted syphilis in a premarital liaison with a prostitute , and had unwittingly passed the condition on to his wife , which would have infected his son . <eol> While <unk> with the loss of her child , Isabella continued to work at The Englishwoman 's Domestic Magazine . Although she was not a regular cook , she and Samuel obtained recipes from other sources . A request to receive the readers ' own recipes led to over 2 @,@ 000 being sent in , which were selected and edited by the Beetons . Published works were also copied , largely <unk> to any of the sources . These included Eliza Acton 's Modern Cookery for Private Families , Elizabeth <unk> 's The <unk> English <unk> , Marie @-@ Antoine <unk> 's Le <unk> royal <unk> , Louis <unk> <unk> 's The French Cook , Alexis <unk> 's The Modern <unk> or , <unk> and The <unk> , Hannah <unk> 's The Art of Cookery Made Plain and Easy , Maria Eliza <unk> 's A New System of Domestic Cookery , and the works of Charles <unk> <unk> . <unk> Daly and Ross G. <unk> , in their examination of Victorian cooking culture , consider that the plagiarism makes it \" an important index of mid @-@ Victorian and middle @-@ class society \" because the production of the text from its own readers ensures that it is a reflection of what was actually being cooked and eaten at the time . In copying the recipes of others , Isabella was following the recommendation given to her by <unk> English , a family friend , who wrote that \" Cookery is a Science that is only learnt by Long Experience and years of study which of course you have not had . Therefore my advice would be compile a book from receipts from a Variety of the Best Books published on Cookery and Heaven knows there is a great variety for you to choose from . \" <eol> The Beetons partly followed the layout of Acton 's recipes , although with a major alteration : whereas the earlier writer provided the method of cooking followed by a list of the required ingredients , the recipes in The Englishwoman 's Domestic Magazine listed the components before the cooking process . Isabella 's standardised layout used for the recipes also showed the approximate costs of each serving , the <unk> of the ingredients and the number of portions per dish . According to the twentieth @-@ century British cookery writer Elizabeth David , one of the strengths of Isabella 's writing was in the \" clarity and details of her general instructions , her brisk comments , her no @-@ nonsense <unk> \" . Margaret Beetham , the historian , sees that one of the strengths of the book was the \" consistent principle of organisation which made its heterogeneous contents look uniform and orderly \" , and brought a consistent style in presentation and layout . Whereas Daly and <unk> consider such an approach as \" nothing if not formulaic \" , Hughes sees it as \" the thing most beloved by the mid <unk> , a system \" . <eol> During the particularly bitter winter of 1858 – 59 Isabella prepared her own soup that she served to the poor of Pinner , \" <unk> for benevolent purposes \" ; her sister later recalled that Isabella \" was busy making [ the ] soup for the poor , and the children used to call with their cans regularly to be <unk> \" . The recipe would become the only entry in her Book of Household Management that was her own . After two years of miscarriages , the couple 's second son was born in June 1859 ; he was also named Samuel Orchart Beeton . Hughes sees the miscarriages as further evidence of Samuel 's syphilis . <eol> As early as 1857 the Beetons had considered using the magazine columns as the basis of a book of collected recipes and <unk> advice , Hughes believes , and in November 1859 they launched a series of 48 @-@ page monthly supplements with The Englishwoman 's Domestic Magazine . The print block for the whole series of the supplements was set from the beginning so the break between each edition was fixed at 48 pages , regardless of the text , and in several issues the text of a sentence or recipe is split between the end of one instalment and the beginning of the next . <eol> The Beetons decided to revamp The Englishwoman 's Domestic Magazine , particularly the fashion column , which the historian Graham Nown describes as \" a rather drab piece \" . They travelled to Paris in March 1860 to meet Adolphe <unk> , the publisher of the French magazine Le Moniteur de la Mode . The magazine carried a full @-@ sized dress pattern outlined on a fold @-@ out piece of paper for users to cut out and make their own dresses . The Beetons came to an agreement with <unk> for the Frenchman to provide patterns and illustrations for their magazine . The first edition to carry the new feature appeared on 1 May , six weeks after the couple returned from Paris . For the redesigned magazine , Samuel was joined as editor by Isabella , who was described as \" <unk> \" . As well as being co @-@ editors , the couple were also equal partners . Isabella brought an efficiency and strong business <unk> to Samuel 's normally disorganised and financially <unk> approach . She joined her husband at work , travelling daily by train to the office , where her presence caused a stir among <unk> , most of whom were male . In June 1860 Isabella and Samuel travelled to Killarney , Ireland , for a fortnight 's holiday , leaving their son at home with his nurse . The Beetons enjoyed the <unk> , although on the days it <unk> , they stayed inside their hotel and worked on the next edition of The Englishwoman 's Domestic Magazine . Isabella was impressed with the food they were served , and wrote in her diary that the <unk> were \" conducted in quite the French style \" . <eol> In September 1861 the Beetons released a new , weekly publication called The Queen , the Ladies ' Newspaper . With the Beetons busy running their other titles , they employed Frederick Greenwood as the editor . <eol> <eol> = = = Mrs Beeton 's Book of Household Management and later , 1861 – 65 = = = <eol> <eol> The complete version of Mrs Beeton 's Book of Household Management , consisting of the 24 collected monthly instalments , was published on 1 October 1861 ; it became one of the major publishing events of the nineteenth century . Isabella included an extensive 26 @-@ page \" Analytical Index \" in the book . Although not an innovation — it had been used in The Family Friend magazine since 1855 — Hughes considers the index in the Book of Household Management to be \" <unk> detailed and <unk> cross @-@ referenced \" . Of the 1 @,@ 112 pages , over 900 contained recipes . The remainder provided advice on fashion , child care , animal <unk> , poisons , the management of servants , science , religion , first aid and the importance in the use of local and seasonal produce . In its first year of publication , the book sold 60 @,@ 000 copies . It reflected Victorian values , particularly hard work , <unk> and <unk> . Christopher <unk> , in his study of the British middle classes , sees that Isabella \" reflected better than anyone else , and for a larger audience , the optimistic message that mid @-@ Victorian England was filled with opportunities for those who were willing to learn how to take advantage of them \" . The food writer <unk> Hope thinks that \" one can understand its success . If ... young ladies knew nothing of domestic arrangements , no better book than this could have been devised for them . \" <eol> The reviews for Book of Household Management were positive . The critic for the London Evening Standard considered that Isabella had earned herself a household reputation , remarking that she had \" succeeded in producing a volume which will be , for years to come , a treasure to be made much of in every English household \" . The critic for the Saturday Review wrote that \" for a really valuable repertory of hints on all sorts of household matters , we recommend Mrs Beeton with few <unk> \" . The anonymous reviewer for The Bradford Observer considered that \" the information afforded ... appears <unk> and explicit \" ; the reviewer also praised the layout of the recipes , highlighting details relating to ingredients , <unk> and the times needed . Writing in The Morning Chronicle , an anonymous commentator opined that \" Mrs Beeton has omitted nothing which tends to the comfort of <unk> , or facilitates the many little troubles and cares that fall to the lot of every wife and mother . She may safely predict that this book will in future take precedence of every other on the same subject . \" For the 1906 edition of the book , The Illustrated London News 's reviewer considered the work \" a formidable body of domestic doctrine \" , and thought that \" the book is almost of the first magnitude \" . <eol> Samuel 's business decisions from 1861 were <unk> and included an ill @-@ advised investment in purchasing paper — in which he lost £ 1 @,@ 000 — and a court case over unpaid bills . His <unk> in business affairs brought on financial difficulties and in early 1862 the couple had moved from their comfortable Pinner house to premises over their office . The air of central London was not conducive to the health of the Beetons ' son , and he began to <unk> . Three days after Christmas his health worsened and he died on New Year 's Eve 1862 at the age of three ; his death certificate gave the cause as \" suppressed <unk> \" and \" <unk> \" . In March 1863 Isabella found that she was pregnant again , and in April the couple moved to a house in <unk> , Kent ; their son , who they named Orchart , was born on New Year 's Eve 1863 . Although the couple had been through financial problems , they enjoyed relative prosperity during 1863 , boosted by the sale of The Queen to Edward Cox in the middle of the year . <eol> In the middle of 1864 the Beetons again visited the <unk> in Paris — the couple 's third visit to the city — and Isabella was pregnant during the visit , just as she had been the previous year . On her return to Britain she began working on an abridged version of the Book of Household Management , which was to be titled The Dictionary of Every @-@ Day Cookery . On 29 January 1865 , while working on the proofs of the dictionary , she went into labour ; the baby — Mayson Moss — was born that day . Isabella began to feel <unk> the following day and died of <unk> fever on 6 February at the age of 28 . <eol> Isabella was buried at West Norwood Cemetery on 11 February . When The Dictionary of Every @-@ Day Cookery was published in the same year , Samuel added a tribute to his wife at the end : <eol> Her works speak for themselves ; and , although taken from this world in the very height and strength , and in the early days of womanhood , she felt satisfaction — so great to all who strive with good intent and warm will — of knowing herself regarded with respect and gratitude . <eol> <eol> = = Legacy = = <eol> <eol> In May 1866 , following a severe downturn in his financial fortunes , Samuel sold the rights to the Book of Household Management to Ward , Lock and Tyler ( later Ward Lock & Co ) . The writer Nancy Spain , in her biography of Isabella , reports that , given the money the company made from the Beetons ' work , \" surely no man ever made a worse or more impractical <unk> \" than Samuel did . In subsequent publications Ward Lock suppressed the details of the lives of the Beetons — especially the death of Isabella — in order to protect their investment by letting readers think she was still alive and creating recipes — what Hughes considers to be \" intentional censorship \" . Those later editions continued to make the connection to Isabella in what Beetham considers to be a \" fairly ruthless marketing policy which was begun by Beeton but carried on vigorously by Ward , Lock , and Tyler \" . Those subsequent volumes bearing Isabella 's name became less reflective of the original . Since its initial publication the Book of Household Management has been issued in numerous <unk> and paperback editions , translated into several languages and has never been out of print . <eol> Isabella and her main work have been subjected to criticism over the course of the twentieth century . Elizabeth David complains of recipes that are \" sometimes <unk> and misleading \" , although she acknowledges that <unk> <unk> 's <unk> <unk> also contains errors . The television cook Delia Smith admits she was puzzled \" how on earth Mrs Beeton 's book managed to utterly eclipse ... [ Acton 's ] superior work \" , while her fellow chef , Clarissa Dickson Wright , opines that \" It would be unfair to blame any one person or one book for the decline of English cookery , but Isabella Beeton and her ubiquitous book do have a lot to answer for . \" In comparison , the food writer Bee Wilson opines that disparaging Isabella 's work was only a \" fashionable \" stance to take and that the cook 's writing \" simply makes you want to cook \" . Christopher Driver , the journalist and food critic , suggests that the \" relative stagnation and want of refinement in the indigenous cooking of Britain between 1880 and 1930 \" may instead be explained by the \" progressive <unk> under successive editors , <unk> and <unk> \" . David comments that \" when plain English cooks \" were active in their kitchens , \" they followed plain English recipes and chiefly those from the Mrs Beeton books or their derivatives \" . Dickson Wright considers Beeton to be a \" fascinating source of information \" from a social history viewpoint , and <unk> and <unk> consider the work to be \" the best and most reliable guide for the scholar to the domestic history of the mid @-@ Victorian era \" . <eol> Despite the criticism , <unk> observes that \" ' Mrs. Beeton ' has ... been for over a century the standard English cookbook , frequently <unk> every other book but the Bible \" . According to the Oxford English Dictionary , the term Mrs Beeton became used as a generic name for \" an authority on cooking and domestic subjects \" as early as 1891 , and Beetham opines that \" ' Mrs. Beeton ' became a trade mark , a brand name \" . In a review by Gavin Koh published in a 2009 issue of The <unk> , Mrs Beeton 's Book of Household Management was labelled a medical classic . In Isabella 's \" attempt to educate the average reader about common medical complaints and their management \" , Koh argues , \" she preceded the family health guides of today \" . Robin <unk> , a professor of strategic management , believes that Isabella 's advice and guidance on household management can also be applied to business management , and her lessons on the subject have stood the test of time better than some of her advice on cooking or <unk> . <eol> Following the radio broadcast of Meet Mrs. Beeton , a 1934 comedy in which Samuel was portrayed in an <unk> light , and Mrs Beeton , a 1937 documentary , <unk> Beeton worked with H. Montgomery Hyde to produce the biography Mr and Mrs Beeton , although completion and publication were delayed until 1951 . In the meantime Nancy Spain published Mrs Beeton and her Husband in 1948 , updated and retitled in 1956 to The Beeton Story . In the new edition Spain hinted at , but did not <unk> upon , on the possibility that Samuel contracted syphilis . Several other biographies followed , including from the historian Sarah Freeman , who wrote Isabella and Sam in 1977 ; Nown 's Mrs Beeton : 150 Years of Cookery and Household Management , published on the 150th anniversary of Isabella 's birthday , and Hughes 's The Short Life and Long Times of Mrs Beeton , published in 2006 . Isabella was ignored by the Dictionary of National Biography for many years : while Acton was included in the first published volume of 1885 , Isabella did not have an entry until 1993 . <eol> There have been several television broadcasts about Isabella . In 1970 Margaret <unk> portrayed her in a solo performance written by Rosemary Hill , in 2006 Anna <unk> played Isabella in a <unk> , and Sophie Dahl presented a documentary , The <unk> Mrs Beeton , in the same year . <eol> The literary historian Kate Thomas sees Isabella as \" a powerful force in the making of middle @-@ class Victorian domesticity \" , while the Oxford University Press , advertising an abridged edition of the Book of Household Management , considers Isabella 's work a \" founding text \" and \" a force in shaping \" the middle @-@ class identity of the Victorian era . Within that identity , the historian Sarah Richardson sees that one of Beeton 's achievements was the integration of different threads of domestic science into one volume , which \" <unk> [ ed ] the middle @-@ class female housekeeper 's role ... placing it in a broader and more public context \" . Nown quotes an unnamed academic who thought that \" Mrs <unk> has preserved the family as a social unit , and made social reforms a possibility \" , while Nicola Humble , in her history of British food , sees The Book of Household Management as \" an engine for social change \" which led to a \" new cult of domesticity that was to play such a major role in mid @-@ Victorian life \" . Nown considers Isabella <eol> ... a singular and remarkable woman , praised in her lifetime and later forgotten and ignored when a pride in light pastry ... were no longer considered <unk> for womanhood . Yet in her lively , progressive way , she helped many women to overcome the loneliness of marriage and gave the family the importance it deserved . In the climate of her time she was brave , strong @-@ minded and a <unk> champion of her sisters everywhere . <eol> <eoa> \n",
      " = Battle of <unk> = <eol> <eol> The Battle of <unk> ( also known as the Battle of <unk> , Battle of <unk> and Battle of <unk> ; German : <unk> <unk> <unk> ) , on 11 November 1805 was an engagement in the Napoleonic Wars during the War of the Third Coalition . <unk> ( modern <unk> ) is located in the <unk> Valley , on the River <unk> , 73 kilometers ( 45 mi ) upstream from Vienna , Austria . The river makes a crescent @-@ shaped curve between <unk> and nearby <unk> an der <unk> and the battle was fought in the flood plain between the river and the mountains . <eol> At <unk> a combined force of Russian and Austrian troops trapped a French division commanded by <unk> <unk> <unk> . The French division was part of the newly created VIII Corps , the so @-@ called Corps <unk> , under command of <unk> <unk> . In pursuing the Austrian retreat from <unk> , <unk> had over @-@ extended his three divisions along the north bank of the <unk> . Mikhail <unk> <unk> , commander of the Coalition force , <unk> <unk> to send <unk> 's division into a trap and French troops were caught in a valley between two Russian columns . They were rescued by the timely arrival of a second division , under command of Pierre <unk> de l <unk> . The battle extended well into the night . Both sides claimed victory . The French lost more than a third of their participants , and <unk> 's division experienced over 40 percent losses . The Austrians and Russians also had heavy <unk> to 16 <unk> perhaps the most significant was the death in action of Johann Heinrich von <unk> , one of Austria 's most capable chiefs of staff . <eol> The battle was fought three weeks after the Austrian capitulation at Ulm and three weeks before the Russo @-@ Austrian defeat at the Battle of <unk> . After <unk> Austria withdrew from the war . The French demanded a high indemnity and Francis II abdicated as Holy Roman Emperor , releasing the German states from their allegiance to the Holy Roman Empire . <eol> <eol> = = Background = = <eol> <eol> In a series of conflicts from 1803 @-@ 15 known as the Napoleonic Wars , various European powers formed five coalitions against the First French Empire . Like the wars sparked by the French Revolution ( 1789 ) , these further revolutionized the formation , organization and training of European armies and led to an unprecedented <unk> , mainly due to mass conscription . Under the leadership of Napoleon , French power rose quickly as the Grande <unk> conquered most of Europe , and collapsed rapidly after the disastrous invasion of Russia in 1812 . Napoleon 's empire ultimately suffered complete military defeat in the 1813 – 14 campaigns , resulting in the restoration of the Bourbon monarchy in France . Although Napoleon made a spectacular return in 1815 , known as the Hundred Days , his defeat at the Battle of Waterloo , the pursuit of his army and himself , his abdication and banishment to the Island of Saint Helena concluded the Napoleonic Wars . <eol> <eol> = = <unk> campaign = = <eol> <eol> From 1803 @-@ 06 the Third Coalition fought the First French Empire and its client states ( see table at right ) . Although several naval battles determined control of the seas , the outcome of the war was decided on the continent , predominantly in two major land operations in the <unk> valley : the Ulm campaign in the upper <unk> and the Vienna campaign , in the middle <unk> valley . <eol> Political conflicts in Vienna delayed Austria 's entry into the Third Coalition until 1805 . After hostilities of the War of the Second Coalition ended in 1801 , <unk> <unk> emperor 's <unk> advantage of the subsequent years of peace to develop a military restructuring plan . He carefully put this plan into effect beginning in 1803 – 04 , but implementation was incomplete in 1805 when Karl Mack , Lieutenant Field Marshal and <unk> @-@ General of the Army , implemented his own restructuring . Mack bypassed Charles ' methodical approach . <unk> in the field , Mack 's plan also undermined the overall command and <unk> structure . Regardless , Mack sent an enthusiastic report to Vienna on the military 's readiness . Furthermore , after <unk> Napoleon 's maneuvers in <unk> , Mack also reported to Vienna on the weakness of French <unk> . His reports convinced the war party advising the emperor , Francis II , to enter the conflict against France , despite Charles ' own advice to the contrary . Responding to the report and rampant anti @-@ French fever in Vienna , Francis dismissed Charles from his post as <unk> and appointed his <unk> brother @-@ in @-@ law , <unk> Ferdinand , as commander . <eol> The inexperienced Ferdinand was a poor choice of replacement for the capable Charles , having neither maturity nor aptitude for the assignment . Although Ferdinand retained nominal command , day @-@ to @-@ day decisions were placed in the hands of Mack , equally ill @-@ suited for such an important assignment . When Mack was wounded early in the campaign , he was unable to take full charge of the army . Consequently , command further devolved to Lieutenant Field Marshal Karl Philipp , Prince of <unk> , an able cavalry officer but inexperienced in the command of such a large army . <eol> <eol> = = = Road to Ulm = = = <eol> <eol> The campaign in the upper <unk> valley began in October , with several clashes in <unk> . Near the Bavarian town of <unk> , 40 kilometers ( 25 mi ) northwest of <unk> , on 8 October the 1st Regiment of <unk> , part of <unk> 's Reserve Cavalry Corps , and grenadiers of <unk> ' V Corps surprised an Austrian force half its size . The Austrians were <unk> in a line and unable to form their defensive squares quickly enough to protect themselves from the 4 @,@ 000 <unk> and 8 @,@ 000 grenadiers . Nearly 3 @,@ 000 Austrians were captured and over 400 were killed or wounded . A day later , at another small town , <unk> south of the <unk> <unk> French 59th Regiment of the Line stormed a bridge over the <unk> and , <unk> , chased two large Austrian columns toward Ulm . <eol> The campaign was not entirely bad news for Vienna . At <unk> , Johann von <unk> arranged his 25 @,@ 000 infantry and cavalry in a prime defensive position and , on 11 October , the overly confident General of Division Pierre <unk> de l <unk> attacked <unk> 's force with fewer than 8 @,@ 000 men . The French lost 1 @,@ 500 men killed and wounded . Aside from taking the Imperial Eagles and <unk> of the 15th and 17th <unk> , <unk> 's force also captured 900 men , 11 guns and 18 ammunition wagons . <eol> <unk> 's victory was a singular success . On 14 October Mack sent two columns out of Ulm in preparation for a breakout to the north : one under Johann <unk> <unk> headed toward <unk> to secure the bridge there , and the other under Franz von <unk> went north with most of the heavy artillery . <unk> the opportunity , Marshal Michel <unk> <unk> the rest of his VI Corps forward to re @-@ establish contact with <unk> , who was still north of the <unk> . In a two @-@ <unk> attack <unk> sent one division to the south of <unk> on the right bank of the <unk> . This division began the assault at <unk> . At the same time another division crossed the river to the east and moved west against <unk> 's position . After clearing Austrian <unk> from a bridge , the French attacked and captured a strategically located <unk> at the top of the hill at bayonet point . The Austrian cavalry unsuccessfully tried to fend off the French , but the Austrian infantry broke and ran . In this engagement alone , the Austrians lost more than half their reserve artillery park , 6 @,@ 000 ( out of 8 @,@ 000 total participants ) dead , wounded or captured and four colors . <unk> 's column also failed to destroy the bridges across the <unk> . <eol> Napoleon 's lightning campaign exposed the Austrian <unk> command structure and poor supply apparatus . Mack completely <unk> the French <unk> and scattered his forces ; as the French defeated each unit separately , the surviving Austrians withdrew toward the Ulm fortifications . Napoleon arrived to take personal command of close to 80 @,@ 000 men . At Ulm on 16 October Karl Mack surrendered his <unk> army of 20 @,@ 000 infantry and 3 @,@ 273 cavalry . The officers were released on the condition that they not serve against France until formally exchanged for French officers captured by the Austrians , an agreement to which they held . <eol> <eol> = = Prelude to battle = = <eol> <eol> The few Austrian corps not trapped at Ulm withdrew toward Vienna , with the French in close pursuit . A Russian army under Gen. Mikhail <unk> also <unk> away from the French , withdrawing to the east . At the <unk> river on 22 October it joined with the retreating Austrian corps commanded by Michael von <unk> . On 5 November the Coalition forces held a successful rearguard action in <unk> . On 7 November the Russians arrived in St. <unk> and crossed the <unk> river the next day . Late on 9 November they destroyed the bridges across the <unk> , holding the last one at the hamlet of Stein , near the village <unk> , until the late afternoon . <eol> <eol> = = = Battlefield = = = <eol> <eol> To the east of Stein , 2 kilometers ( 1 @.@ 2 mi ) down an old road , lay <unk> , with its small population of a few hundred , at the confluence of the stream of that name and the <unk> . To the west of Stein the <unk> made a large curve , creating a crescent @-@ shaped floodplain between it and the mountains . At the far western end of the floodplain , where the mountains came down almost to the river 's edge , was <unk> with its castle , known as <unk> <unk> . The castle had served as a prison for Richard I of England in <unk> . In 1645 – 46 , during the Thirty Years War , the Swedes had fortified the castle and then demolished it when they withdrew . It stands at 159 meters ( 522 ft ) , on the highest ridge of a mountain <unk> with <unk> and pinnacles of granite . Because the mountain was sparsely <unk> , it was difficult to distinguish the ruins from the rocks . <unk> canyons cut through the mountain , and widen into the plain below . Between <unk> and Stein , on the flood plain , lay the hamlets of <unk> and <unk> . Near the hamlets , the <unk> flood plain was at its <unk> , extending at the most 762 meters ( 2 @,@ 500 ft ) from the base of the <unk> mountain to the bank of the river . <eol> The region was known for its wine . Since the 15th century the local inhabitants practiced <unk> and the wine producers formed St. Paul <unk> ' Guild in <unk> , the oldest such guild in the German @-@ speaking world . <unk> vineyards extended up the sides of the <unk> River until it became a mountain stream and terrain was unsuitable for cultivation . The <unk> plain supported both <unk> and agriculture . As the terrain became <unk> , the vines grew in terraces built from the dark <unk> , primordial rock . From <unk> to <unk> the river makes its wide curve ; the mountains and the steeply terraced slopes prevent clear line @-@ of @-@ sight between the two towns . <eol> <eol> = = = <unk> = = = <eol> <eol> Napoleon had calculated that <unk> would withdraw toward Vienna , expecting reinforcements from Russia ; he envisioned that the armies would engage in a great battle at Vienna , and that this battle would decide the war . Consequently , Napoleon drew divisions from four of the other seven corps of the Grande <unk> to create a new VIII Corps . This corps was to secure the north shore of the <unk> , block any of the Austrian or Russian groups from reinforcing one another and , more importantly , prevent <unk> from crossing the river and escaping to Russia . <eol> The new VIII Corps , under the overall command of <unk> <unk> , included three infantry divisions and a division of cavalry ( see Order of Battle below ) . Corps <unk> , as it was known , crossed the <unk> at <unk> and <unk> in early November 1805 and marched east , on the north bank of the <unk> . Operating independently , the <unk> 's cavalry conducted reconnaissance ahead of them and on the flanks . Gen. <unk> 's division ( about 6 @,@ 000 men ) took the lead ; <unk> was with them . They were followed by Gen. <unk> 's division ( another 4 @,@ 000 ) about one day 's march behind . Jean @-@ Baptiste <unk> 's division ( another 4 @,@ 000 ) , marching another day behind <unk> , brought up the rear . A flotilla of 50 boats acquired at <unk> provided communications across the <unk> . Before sending <unk> on his mission , Napoleon instructed him to protect his north flank at all times against possible Russian reinforcements , advice he reiterated in subsequent written orders . Napoleon also advised <unk> to secure all <unk> of the <unk> between <unk> and Vienna . <eol> On 9 November <unk> 's division reached <unk> an der <unk> and covered the 50 kilometers ( 31 mi ) to <unk> by early on the following afternoon . Here it <unk> with some Russian patrols to the east of the town and expelled them . Feeling confident , the French established a forward post just upstream from Stein . In <unk> itself <unk> set up his command post and directed the establishment of a small field hospital . Although the position seemed secure , he had ignored Napoleon 's strict instructions and neglected to protect his left ( north ) flank . <eol> This failure was an important factor when <unk> lost his corps ' so @-@ called \" eyes \" : after he and <unk> had crossed the <unk> , the French <unk> had <unk> to the northwest , leaving only three squadrons of the 4th <unk> available for reconnaissance . These had left the division and were operating independently of <unk> 's command . Consequently , <unk> and <unk> marched <unk> through the narrow canyon west of <unk> , not knowing what lay ahead of them . <unk> had led the Coalition army across the <unk> at <unk> , a short distance past Stein , and destroyed the bridge behind him . His actions deprived the French commanders of a possible route across the <unk> , putting the deployment of the entire French division at further risk in the case of retreat . In this decision <unk> abandoned Vienna to the French , who were <unk> on the Austrian capital from the north , west and southwest , for the security of uniting with reinforcements from <unk> . <unk> chose a military solution over a political one . <eol> Unknown to either <unk> or <unk> , the Coalition had concentrated a force of approximately 24 @,@ 000 men ( mostly Russians and a few Austrians ) within a few kilometers of the French position at <unk> . In comparison , <unk> 's division had only 6 @,@ 000 men . The Austro @-@ Russian force was a mixture of infantry , <unk> ( usually deployed as <unk> ) , Russian <unk> and Russian and Austrian cavalry , accompanied by more than 68 artillery pieces . <unk> , who had learned the military arts under the tutelage of the legendary Russian <unk> <unk> , had overall command . The Russian cavalry , units of the greatly feared Cossacks , were well @-@ suited for patrolling the river bank ; indeed , on 9 November they had taken 40 French soldiers as prisoners . Furthermore , reinforcements stood in Moravia , less than two weeks ' march away . If the main body of the French army crossed the river , they would require time to prepare . <unk> would have ample warning of any large @-@ scale French movement . <eol> After the afternoon 's initial <unk> with the French , <unk> held a council of war on the evening of 10 November at <unk> , at the great <unk> there . He knew several things . First , he knew the positions of the French from prisoners his Cossacks had captured . He also knew that <unk> had crossed at <unk> and was well ahead of any French reinforcements : <unk> had crossed at <unk> and , by 10 November , stood at <unk> , 50 kilometers ( 31 mi ) upstream , and <unk> was another 7 kilometers ( 4 mi ) further behind him . <unk> knew the size of the French <unk> division <unk> its positions , and he knew that most of the <unk> were not covering the French flank but had turned north . He also knew , or had made a good <unk> , about Napoleon 's orders , so he knew what to offer <unk> and <unk> as bait . <eol> <eol> = = = Battle plan = = = <eol> <eol> In addition to the Russian generals , the council included Austrian commanders Lieutenant Field Marshal Johann Heinrich von <unk> and Friedrich Karl Wilhelm , <unk> <unk> <unk> . <unk> , who had retired from the military in 1800 , had been recalled into service after the Ulm <unk> and had come to <unk> highly recommended by the Emperor . He was an experienced <unk> and strategist and had served in a variety of posts in the Habsburg military ; he had been <unk> Charles ' trusted adviser during the campaigns from 1796 to 1800 and had assisted in planning several of Charles ' victories . Upon his recall , <unk> was appointed Chief of the <unk> General Staff of the Coalition Army . The generals had found among the Austrian force one <unk> Christoph Freiherr von <unk> ( 1753 – 1824 ) , who had knowledge of the local geography . <eol> Together , <unk> , <unk> and the other generals , with von <unk> 's advice on the local terrain , <unk> a plan to encircle the French at <unk> . Russian commander Mikhail <unk> <unk> would approach <unk> 's division from the east , supported by <unk> <unk> 's corps , and pin the French in place . Three additional columns , commanded by <unk> <unk> ( <unk> ) , <unk> Gen. <unk> and <unk> , would outflank the French from the west and the north . They would offer , as bait , a rumor : the Russian army was retreating into Moravia and only a rearguard would be left at <unk> . <eol> <eol> = = Battle = = <eol> <eol> On the night of 10 – 11 November a Russian column under <unk> 's command began its passage through the narrow canyons , intent on arriving at <unk> by noon ; two more columns , under <unk> and <unk> , moved in wider <unk> , planning to pass through the mountains and attack the French , who were extended along the river bank . According to the plan , in late morning <unk> 's column would emerge from the mountains first and launch a flanking assault on the French right . This flanking attack , combined with <unk> 's frontal assault from Stein , would force the French into a <unk> ; <unk> , they would have no option but to <unk> die . To ensure the success of the plan , the second and third columns , under <unk> and <unk> , would arrive in early and mid @-@ afternoon and support the earlier assaults . In this way , even if the French tried to retreat west to <unk> , they would not escape the <unk> @-@ like grip of the Coalition army . <eol> <unk> accepted the bait of a <unk> Russian retreat . In the early morning of 11 November he and <unk> departed from <unk> to seize Stein and <unk> , <unk> the Russians had either abandoned the settlements or left only a small rear @-@ guard behind . As they approached Stein , a column of <unk> 's troops attacked the French forward positions . <unk> this force was the <unk> Russian rear guard , <unk> ordered Gen. <unk> to counterattack and push east towards the town of Stein . Fighting spread though the villages of <unk> , <unk> and the farm at <unk> . Instead of withdrawing , as a rear guard would , more and more Russian troops appeared and engaged the French column . <eol> Initially <unk> made rapid progress , but he quickly recognized that the opposing force was much stronger than the typical rear guard of a retreating army . Realizing he had been <unk> and that <unk> 's troops were tiring rapidly , <unk> sent orders to <unk> 's division to <unk> forward . By mid @-@ morning the French momentum had stalled ; <unk> committed most of his remaining forces to driving <unk> back , leaving a single <unk> 300 <unk> cover his northern flank , and sent the rest to attack the Russian right . Within 30 minutes he achieved the superiority of numbers he sought . His 4 @,@ 500 French opposed 2 @,@ 600 Russians and forced them back toward Stein while pressing an attack along the river . <unk> had no option , for neither <unk> 's nor <unk> 's flanking columns were to be seen . <eol> At this stage of the battle fighting paused . <unk> and <unk> waited for <unk> 's arrival while <unk> and <unk> waited for <unk> 's and <unk> 's . <unk> 's column was expected to be the last to join the fight because it had to march the greatest distance . The timing of the respite varies , depending on whose reports are consulted : fighting paused at around 12 : 00 or 14 : 00 . <unk> arrived first and immediately assaulted <unk> 's line with three battalions , pushing the French out of <unk> . Caught between two strong forces , <unk> attempted to push his way back through <unk> , to reach the river where the flotilla could evacuate his exhausted troops . <unk> through the narrow <unk> canyon and fighting off the Russian force at their rear , <unk> and his division were trapped when more of <unk> 's Russians appeared to block their retreat . The narrow <unk> hampered the Russians ; <unk> 's men had to march out of the canyons , form ranks and attack in waves . Despite <unk> 's continuous assault in the next two to three hours , <unk> and <unk> pushed the Russians back up the narrow <unk> in the hillside . At this point , <unk> 's column appeared behind the French line and joined the battle . The French were outnumbered more than three to one , assaulted in the front by <unk> 's column , in the middle by <unk> 's and in the rear by <unk> . <eol> Earlier in the morning <unk> had proceeded with his column south and east along the river , from <unk> , according to instructions . Even before the arrival of <unk> 's <unk> , he heard the sound of artillery in the distance and sent riders ahead to discover the cause . They came back to report that a Russian column ( <unk> 's ) was descending from the mountains to take the road to <unk> . Realizing this would separate him from the forward division , <unk> <unk> his troops toward the sound of battle and deployed them to take the Russians in the flank . The French assault , heralded by cannon fire , caused <unk> 's troops to turn their attention from <unk> 's <unk> force to face these new <unk> . Although superior in numbers , <unk> 's column had no supporting artillery , and the narrow space prevented them from taking advantage of their size . It was <unk> 's turn to face attackers at his front and rear , until the arrival of <unk> 's column , which had <unk> its way through the mountains in the west . <eol> <unk> arrived at dusk , and the action continued well after dark ; in mid @-@ November night falls at close to 17 : 00 in the upper <unk> <unk> . Despite the darkness , <unk> descended out of the <unk> and deployed his troops to <unk> <unk> 's flank . As his Russians entered the <unk> , they came between a battalion of French and another of Russians . With the additional force , the French were overwhelmed , but most of the shooting subsided when the combatants could not tell apart friend from <unk> in the dark . Under the cover of darkness , <unk> used the French flotilla to evacuate his exhausted troops to the south bank . The French and Russians continued to <unk> <unk> into the night as <unk> encountered one another in the dark . <unk> of <unk> 's force provided any necessary rear guard action , and the following morning the remaining men were evacuated from the north shore of the <unk> , while they maintained possession of only <unk> and <unk> on the north bank . <eol> <eol> = = = Losses = = = <eol> <eol> The losses were <unk> : <unk> lost close to 40 percent of his division to death and wounds . Aside from losing five guns , 47 officers and 895 men under his command were captured , bringing the loss of <unk> closer to 60 percent ; furthermore , he lost the eagles of the 4th Infantry Regiment ( France ) and the eagle and <unk> of the 4th <unk> . The Russians lost around 4 @,@ 000 , about 16 percent of their force , and two regimental colors . The Austrian Lieutenant Field Marshal <unk> was killed as the battle concluded , probably by Russian <unk> in the confused melee . The vineyards and the villages of <unk> and <unk> were destroyed , as was most of <unk> and Stein . <unk> was heavily damaged ; the French plundered the town at least twice , and \" <unk> handled \" its inhabitants . <eol> <eol> = = Aftermath = = <eol> <eol> Both sides claimed victory . Although losses were fairly equal in terms of <unk> wounded or dead on each <unk> Coalition forces went into battle with 24 @,@ 000 men while the French started with <unk> 's division of about 6 @,@ 000 , which grew close to 8 @,@ 000 when <unk> 's men joined the fighting in the afternoon . Regardless , <unk> 's division was nearly destroyed ; the 30 percent losses experienced by the French fell predominantly on his division . <unk> for both sides , the fighting was hard . The weather had been cold ; an early storm had left slick icy mud in the roadways , and <unk> \" like chandeliers \" hung from the trees . <eol> For the Coalition , the Russians were secure on the north bank of the <unk> , awaiting reinforcements from <unk> ; the bridges between <unk> and Vienna had been destroyed , making French access to the Austrian capital more difficult , but not impossible . After six months of fighting in which the Austrians had enjoyed little good news , the Coalition could claim a difficult and timely victory . The French had retreated from the field with a badly <unk> division and <unk> had secured the right flank . Indeed , Francis was so pleased with the outcome at <unk> that he awarded <unk> the Military Order of Maria <unk> . <eol> For the French , the survival of the Corps <unk> seemed nothing short of a miracle . The remainder of <unk> 's division crossed the river the next morning and eventually <unk> in Vienna , which the French acquired by deception later in the month . More importantly for them , the French force had performed well over difficult terrain and under terrible combat conditions . Initially there had been some panic and parts of at least one French battalion had tried to escape on the flotilla craft . They had lost control of the boats in the current and smashed into the pillars of the burned bridge at <unk> , <unk> their boats . <unk> into the icy river , most had drowned . Despite this initial panic , <unk> 's column retained its cohesion , and responded well to various difficult demands . <unk> had demonstrated his tactical <unk> : when he heard cannon fire , he directed his troops toward it to support the French division . In terms of French staffing , <unk> 's failure to guard his flank , especially in the face of Napoleon 's direct advice , adversely influenced his relationship with his commander . However , in the immediate weeks ahead , the flamboyant <unk> did more to annoy Napoleon than <unk> had . In assessing the battle and its aftermath , historians have laid the blame and credit for its outcome not only on <unk> and <unk> : \" Napoleon , aware of <unk> 's danger and his own <unk> for it , vented his frustration on <unk> , whom he unjustly accused of abandoning <unk> for the empty glory of riding through Vienna . \" <eol> After the victory at <unk> , Napoleon dispersed the VIII Corps and reassigned <unk> . However disappointed he may have been with <unk> , Napoleon was pleased with <unk> 's performance . As recognition of his conduct in what the French called \" the immortal Battle of <unk> \" , <unk> received the Officer 's Grand Cross of the Legion of Honor . <eol> The loss of <unk> was a significant blow to the Austrian military organization . Called out of retirement for this specific task , he was one of their most experienced general staff officers , other than the <unk> Charles . From the summer of 1796 until his retirement in 1800 he had been Chief of the <unk> General Staff of the Army , the Lower Rhine , the Rhine and the Army of Germany . Furthermore , he was a trusted member of <unk> Charles ' staff . He had helped to design several of Charles ' more important victories at <unk> , <unk> , the <unk> at <unk> and <unk> , the battles at <unk> and <unk> , and the northern Swiss Campaign of <unk> that included battles at <unk> and <unk> . An experienced officer and excellent <unk> , he might well have made a more effective Chief of the <unk> General Staff of the Coalition Army at the Battle of <unk> than his eventual replacement , Franz von <unk> . In <unk> 's absence <unk> , the architect of the Austrian catastrophe at <unk> in 1800 , was chosen to develop the general battle plan of Coalition action at <unk> . <unk> , undoubtedly a far better <unk> than <unk> , and possessed of superior training and mapping skills , would have developed a more realistic Coalition plan for <unk> . <unk> 's presence would probably not have been enough to turn that defeat into a victory , but it would have <unk> the magnitude of the Coalition 's losses ; <unk> was considered one of Napoleon 's finest <unk> . <eol> In the broader picture , despite the important major naval engagements , the outcome of the War of the Third Coalition was determined on the Continent , predominantly in the two major land operations . In the Ulm campaign , the Habsburgs achieved some minor victories , such as <unk> 's at <unk> @-@ <unk> , but ultimately lost an entire army and an officer corps . The latter would not resume arms against France until formally exchanged . This condition crippled the Austrian military leadership and forced the recall of such pensioners as <unk> out of retirement . After the capitulation at Ulm , isolated portions of the Austrian military <unk> capture and joined with their Russian allies ; Michael von <unk> 's corps slipped out of the <unk> and joined <unk> 's force . A few other small forces refused to <unk> and seemingly melted into the Bavarian mountains and the <unk> forests , to reappear in Bohemia for <unk> . Sixteen hundred cavalry , including <unk> Ferdinand and Prince <unk> , broke out of Ulm before its capitulation . Maximilian , Count of <unk> , led his column back through the mountains into Austria , fighting rear guard actions against pursuing French forces at the <unk> ( <unk> ) and <unk> . These elusive units were insufficient to balance heavy losses at key battles in which the Austrians could not hold their own against the French . Between the Ulm capitulation and the Austrian and Russian defeat at <unk> , there were other minor achievements : a successful <unk> between the cavalry that escaped from Ulm and the French near the town of <unk> , the contested victory at <unk> , and another within days at <unk> . <eol> The second determining event , the decisive French victory at the Battle of <unk> over the combined Russian and Austrian armies , forced the Austrian withdrawal from the Coalition . The subsequent Peace of <unk> , signed on 26 December 1805 , reinforced the earlier treaties of <unk> <unk> and <unk> . Furthermore , Austria ceded land to Napoleon 's German allies , and paid an indemnity of 40 million francs . Victory at <unk> also gave Napoleon the latitude to create a <unk> zone of German states between France and the states of Prussia , Russia , and Austria . These measures did not establish a lasting peace on the continent . Prussian worries about growing French influence in Central Europe sparked the War of the Fourth Coalition in 1806 , in which Austria did not participate . <eol> <eol> = = Battlefield <unk> = = <eol> <eol> Until 1805 , <unk> was probably best known as the village in which crusader Richard the <unk> was held by Leopold V , Duke of Austria . In 1741 , during the War of the Austrian <unk> , several hundred local villagers had held off the French and Bavarian armies , intent on capturing Vienna , by painting drain pipes to look like cannons , and beating on drums , thus suggesting the presence of a large force . <eol> After 1805 , the exploits of 40 @,@ 000 French , Russian , and Austrian soldiers excited the European imagination . General <unk> 's grave has never been found , but in 1811 a monument for him was erected at the Stein Tor , the gate leading from the old village of <unk> to the hamlet of Stein . The house in which Captain von <unk> lived was marked with a bronze plate commemorating his contribution to the battle . In 1840 , a Spanish <unk> created an image of the battle , which was later expanded by English <unk> John <unk> . The image depicts the evacuation of French troops via the <unk> flotilla ( see <unk> image ) on a <unk> night . In fact , the moon was in its last quarter phase 48 hours later , and on 11 November probably did not provide as much light as depicted in the image . <eol> In 1836 , Jean Antoine <unk> Fort ( French , 1793 – 1861 ) , a historical painter , created a <unk> of the battle , Combat de <unk> le 11 <unk> 1805 ( ( English ) Battle of <unk> of 11 November 1805 ) , which is in the <unk> collection at Versailles . <eol> In the Russian novel War and Peace , Leo <unk> devoted several pages to the battle , its prelude , and its aftermath , and the delivery of its news to the Tsar by Prince Andrew . Between <unk> and <unk> , at the edge of the <unk> plain , stands the \" Little Frenchman \" memorial ( see image ) erected in 1905 to commemorate the battle ; it bears the names of <unk> , <unk> , <unk> , <unk> , and others on a copper @-@ engraved plate . <eol> <eol> = = Orders of battle = = <eol> <eol> <eol> = = = French VIII . Corps ( Corps <unk> ) = = = <eol> <eol> On 6 November , <unk> Adolphe <unk> commanded the following forces : <eol> 1st Division under command of Pierre <unk> de l <unk> ( formerly 1st Division of VI . Corps ) , six battalions , three squadrons , and three guns , most of which were involved in the fighting after mid @-@ day . <eol> 2nd Division under command of <unk> <unk> <unk> <unk> de la <unk> ( formerly 2nd Division of the V. Corps ) , nine battalions , three squadrons , three guns . <eol> 3rd Division under command of Jean @-@ Baptiste <unk> ( <unk> Division , formerly 3rd Division of the II . Corps ) . The 3rd Division was not involved in the fighting . <eol> Dragoon Division under command of Louis Klein . Klein 's division included the 1st , 2nd , 4th , and 14th Regiments of <unk> . They were not involved in the fighting . <eol> <unk> fleet of fifty boats , under the command of Frigate Captain <unk> . <eol> Total : fifteen battalions , six squadrons , six guns , approximately 12 @,@ 000 men , not all of which were involved in the fighting . <eol> <eol> = = = Coalition columns = = = <eol> <eol> First Column , commanded by General of Brigade Prince <unk> <unk> <unk> , included three battalions of infantry , three <unk> battalions , and three <unk> battalions , ten squadrons of Hussars . <eol> Second Column , Lieutenant General <unk> , included six battalions of infantry , three battalions of grenadiers , and five squadrons of Hussars . <eol> Third Column , commanded by Lieutenant General <unk> , including six battalions of infantry , one battalion from the 8th <unk> regiment , and ten squadrons of the <unk> Regiment <unk> . <eol> Fourth Column , commanded by Lieutenant General <unk> , nine battalions of infantry . <eol> Fifth Column , Lieutenant General Freiherr von <unk> , nine battalions of infantry . <eol> Sixth Column , Lieutenant General Freiherr von Rosen , with six battalions of Infantry and ten squadrons of cavalry . The Sixth Column did not take part in the fighting . <eol> Austrian Infantry Brigade , Major General Johann <unk> von <unk> @-@ <unk> , four battalions of Border Infantry , including the highly decorated 9th Regiment <unk> . <eol> Austrian Cavalry Division , Lieutenant Field Marshal Friedrich Karl Wilhelm , <unk> <unk> <unk> , twenty @-@ two squadrons of cavalry . <eol> Total : fifty @-@ eight battalions , sixty @-@ two squadrons , fourteen artillery batteries , approximately 24 @,@ 000 men and 168 guns . <eol> <eoa> \n",
      " = AIL Storm = <eol> <eol> The AIL Storm ( Hebrew : <unk> , <unk> ) is an Israeli manufactured off @-@ road vehicle and the <unk> of the Israeli Security Forces . The series of Jeep Wrangler based vehicles have been produced by Automotive Industries Ltd. in Upper Nazareth under licence from Chrysler since 1990 . The vehicles fill a number of military roles , including that of armoured Infantry <unk> Vehicle , and certain models are available for export as well as for the civilian market . <eol> Production of an updated four @-@ door second generation model commenced in 2006 despite some mixed messages from the Storm 's primary customer , the Israel Defense Forces . Development of a third generation vehicle based on the new Jeep Wrangler JK has been completed and significant production for both Israeli and foreign customers is under way . <eol> <eol> = = Storm I = = <eol> <eol> The M @-@ 240 Storm <unk> Vehicle is the first of three Storm generations . A variant of the 1991 Jeep Wrangler <unk> and the older <unk> @-@ 6 / <unk> @-@ 8 <unk> , it is entirely produced in Israel by Automotive Industries Ltd. with the exception of the engines , as their manufacture is not economically viable on the Storm 's market scale . <eol> The Storm was primarily meant to satisfy Israeli military needs , but capable long and short versions are produced for the local civilian market . Like the Jeep , it has a conventional front @-@ engine design with a driver and passenger seated behind the engine , and room for cargo or passengers behind them . It is powered by an AMC 3 @.@ 983 litre 6 @-@ cylinder in @-@ line petrol with fuel injection developing 180 hp ( 130 kW ) at 4 @,@ 700 rpm , fitted with <unk> 2 @-@ stage air cleaner or a <unk> 2 @.@ 5 litre 4 @-@ cylinder <unk> diesel developing 88 <unk> ( 118 hp ) at 4 @,@ 200 rpm . The front axle is fully floating and the rear axle is semi @-@ floating , while a reinforced frame and body as well as good angles of approach and departure ( 40 ° and 37 ° for short frame , 40 ° and 26 @.@ 5 ° for long frame ) add to the Storm 's off @-@ <unk> capability . <eol> The two production frame lengths , 4 @.@ 15 ( 13 @.@ 6 ) and 4 @.@ 5 metres ( 14 @.@ 8 ft ) , the latter of which was among the few such Jeep @-@ derivatives in production in recent years , were both available in civilian and military models . Aside from the Israeli market , Storms have long been exported to countries in South America , Asia , and Africa . A Jeep @-@ managed production line in Egypt , whose vehicles are used by the Egyptian armed forces , was absorbed into the AIL Storm production after it closed in 1995 . <eol> <eol> = = = Security versions = = = <eol> <eol> Like its parent Jeep Wrangler , the Storm is first and foremost an <unk> , capable <unk> and utility vehicle meant to tackle extreme terrain in a general reconnaissance role , and can be outfitted with a machine gun or other weapons systems . When armed with a 105 mm ( 4 @.@ 1 in ) <unk> recoilless rifle , the vehicle is uniquely capable of firing directly over its blast guard equipped hood rather than in the perpendicular position required by most other vehicles . <eol> A variant of the extended version used in desert border patrol makes use of a high @-@ <unk> canopy to allow a <unk> rear @-@ facing heavy machine @-@ gun mount , while the canopy can be extended to provide a mobile command post . An air conditioned <unk> version of the extended model is often used by officers , and a version developed for riot control has clear <unk> shielding along the rear sides and roof , as well as gunports for less @-@ lethal weapons . The shielding allows for a wide field of view while at the same time protecting against <unk> and rock @-@ throwing . <eol> <eol> = = = Armoured version = = = <eol> <eol> As with several analogous light military vehicles , despite being originally designed to fill a light reconnaissance role , the rise of urban warfare and close quarters combat meant that the Israel Defense Forces had to recast the Storm in new roles . <eol> When the need for a light armoured vehicle became apparent to the Israeli security forces , AIL 's engineering department designed a vehicle protection system from the bottom up , integrating it into the existing vehicle in a manner that did not compromise its off @-@ road and other capabilities , and that did not create the mechanical strain and increase in maintenance often associated with up @-@ <unk> , in part due to its computerized 180 horsepower ( 130 kW ) <unk> engine . <eol> The armour protects against 7 @.@ 62 × <unk> ( 0 @.@ 3 in ) armour @-@ piercing ammunition , and maintains a high protection @-@ to @-@ weight and cost ratio by employing IDF approved advanced materials . The protected Israeli configuration 's gross vehicle weight is 3 @,@ 000 kilograms ( 6 @,@ 614 lb ) , though several varying protection levels are in use with individual units . <eol> Another important asset are the Storm 's narrow dimensions , which allow it to traverse the narrow alleyways common to the <unk> of many Middle Eastern cities , places that armoured <unk> can only enter with great difficulty and minimal <unk> , if at all . Full @-@ height rear doors which allow for the quick deployment of fully equipped troops into combat are <unk> as another advantage over similar vehicles . <eol> <eol> = = = Civilian use = = = <eol> <eol> First generation Storms were made available to the general public in Israel from 1992 to 2001 . A small number were purchased directly by private consumers , while larger numbers were acquired second @-@ hand from Israeli government @-@ owned firms like the Israel Electric Company and <unk> water company , as well as National Parks Authority and Israel Police . <unk> Storms are popular with off @-@ <unk> enthusiasts in Israel . <eol> <eol> = = Storm II = = <eol> <eol> Beginning in 2006 , AIL began delivery of an improved model to the IDF , the M @-@ 242 Storm Mark II , known in the field as the \" Storm Commander \" . A number of significant changes have been incorporated into the new TJ @-@ based Storms stemming from soldiers ' feedback , updated operational requirements , and testing by <unk> Army Headquarters and <unk> , Medical , and the Centers Directorate . Perhaps the most obvious change is the addition of dual passenger doors , making the Storm II the first five @-@ door Jeep Wrangler derivative . <eol> Other improvements include the change to a manual transmission with six forward speeds ( instead of the previous four ) , and increased stability resulting from wider track axles than its predecessor . Leaf springs were replaced with modern coil spring suspension front and rear , and the Storm II features rear Dana 44 axles and front TJ Dana 30s , factory designed slip <unk> <unk> , and the added safety of standard <unk> . Soldiers ' comfort was addressed as well with the addition of standard rear air conditioning and a compact disc player . <eol> Storm II is also produced in an armoured version , and is offered with an optional 2 @.@ 8 litre <unk> <unk> <unk> , automatic transmission , right hand drive , and run @-@ flat tyres . AIL is capable of completing ten vehicles daily . Due to recently passed tax laws , a civilian version is not yet available in the local market . <eol> <eol> = = = MDT David controversy = = = <eol> <eol> Developed in the 2000s at an investment of US $ 2 million after IDF commitments for 1 @,@ 200 units , some AIL jobs were believed to be in jeopardy following a mid @-@ 2005 announcement that the IDF would purchase 100 US sold Land Rover Defender @-@ based MDT David . The announcement provoked threats of protests from AIL 's management and labourers , who had recently faced the blow of local <unk> assembly <unk> due to budget <unk> . The MDT David was chosen over the armoured version of the Storm because the heavy Storm was said to suffer from handling and reliability problems , safety <unk> and limited mission <unk> . However the IDF said that the purchase of the David was to fill a temporary gap in production until the Storm II 's testing was completed , and has since begun filling its commitment . <eol> <eol> = = Storm III = = <eol> <eol> A Storm Mark III was set to be produced for the Israeli defense forces starting in June 2008 , when the IDF was to purchase around 600 vehicles beginning in early 2011 . Based on the then new four @-@ door Jeep Wrangler JK design , the Mark III is meant to address some of the <unk> of the earlier Mark II . Whereas the previous vehicle was an update of the original TK Storm , the Storm III was designed from the outset with a five @-@ door configuration . Unlike the TJ @-@ L , the new JK Storm has a much higher maximum load capacity in part due to heavier @-@ duty shock <unk> , springs and axles , necessary for an armored version . It includes a standard <unk> <unk> <unk> <unk> and automatic transmission . <eol> Like the Storm II , the Mark III was initially available only to the military with versions set to be delivered to the Israel Police in 2009 . A civilian version would only be released if the local tax code was modified to allow it to compete with foreign imports of the same class . AIL states that if such a thing would happen , a petrol engine version could be offered . Regarding <unk> markets , the Storm 3 has already seen use in several countries , especially in its armored version . <eol> A pair of production <unk> 's was tested by Israeli web magazine <unk> journalists in April 2009 . It was dubbed \" probably , the best Jeep ever \" . <eol> <eol> = = = Commander version = = = <eol> <eol> The commander version incorporates a 5 @-@ door hard top cab allowing for the quick and convenient entrance and exit of the driver and all passengers or troops . A large rear compartment enables the storage of both cargo and communications equipment . <eol> This version comes equipped with an air conditioning system providing maximum comfort in hot climatic conditions . A roll over protection structure ( <unk> ) <unk> safety conditions for passengers . <eol> <eol> = = = Armored version = = = <eol> <eol> The armored version of the Storm 3 , designed for protection against light weapon threats , incorporates a heavy duty transfer case and a specially designed suspension system which includes heavy duty springs ( front - coil , rear - leaf ) and shock <unk> , together with rigid heavy duty axles allowing for a smooth and safe ride on both rough terrain as well as regular highways . <eol> <eol> = = = Reconnaissance & Patrol version = = = <eol> <eol> The Storm 3 reconnaissance and patrol model allows for extra stowage of fuel , water and equipment . This version is especially suited to be fitted with various machine gun or special equipment mountings . <eol> <eoa> \n"
     ]
    }
   ],
   "source": [
    "! head -n 5 wikitext-2/train.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qwEl1XIe8ULf"
   },
   "source": [
    "As you can see, every line contains a full article consisting of several paragraphs (`<eol>` token is basically a `\\n`). \n",
    "\n",
    "Here is one article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2585,
     "status": "ok",
     "timestamp": 1588286556500,
     "user": {
      "displayName": "sadra barikbein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCq0YqNHWfcZyly3I_AMTkvS8_tO8PZOQpGtkK=s64",
      "userId": "07513140462530453530"
     },
     "user_tz": -270
    },
    "id": "M9C1PLsxAAok",
    "outputId": "dc473157-2ffb-427c-d596-89605f5f9c72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " = Old Baltimore Pike =\n",
      "\n",
      " Old Baltimore Pike is a road in the U.S. state of Delaware . The road , known as New Castle County Road 26 , runs from Maryland Route 281 ( MD 281 ) at the Maryland state line south of Newark , Delaware and continues east to Christiana , ending near Delaware Route 1 ( DE 1 ) . The road is paralleled by Interstate 95 ( I @-@ 95 , Delaware Turnpike ) to the north and U.S. Route 40 ( US 40 , Pulaski Highway ) to the south . The Old Baltimore Pike was built before 1720 and connected Elkton , Maryland to Christiana . It was a turnpike called the Elk and Christiana Turnpike between 1817 and 1838 . In the past it served as a major connection between Philadelphia and Baltimore .\n",
      "\n",
      " = = Route description = =\n",
      "\n",
      " Old Baltimore Pike begins at the Delaware – Maryland state line near Newark , Delaware , where the highway continues west into that state as MD 281 . The road heads northeast from the state line through wooded residential areas as a two @-@ lane undivided road , intersecting <unk> Chapel Road before passing south of Iron Hill Park . Old Baltimore Pike crosses DE <unk> and enters rural areas . Here , the roadway passes <unk> 's Bridge , a historic battle site of the American Revolutionary War . It then crosses Norfolk Southern 's <unk> Secondary railroad line and encounters DE 72 .\n",
      " After this intersection , Old Baltimore Pike continues past suburban neighborhoods , gaining a center left @-@ turn lane and intersecting Salem Church Road . Farther east , the highway reaches a junction with DE 273 in a wooded area . At this point , the road <unk> to two lanes again and heads into the community of Christiana . Here , the highway intersects DE 7 , where it turns north and follows that route through residential areas . The road comes to an interchange with DE 1 to the west of the Christiana Mall , where DE 7 continues north along with DE 1 and Old Baltimore Pike reaches a dead end .\n",
      "\n",
      " = = History = =\n",
      "\n",
      " The Old Baltimore Pike was built before 1720 . The road was known as the Great Road and ran between Head of Elk ( now Elkton , Maryland ) and Christiana Bridge . It was later known as the Christiana @-@ Elkton Turnpike before becoming Old Baltimore Pike . This path served as a major connection between Philadelphia and Baltimore in addition to providing access between the shipping area of Christiana Bridge and agricultural areas in northern Delaware , northern Maryland , and southeastern Pennsylvania . In 1723 , Welsh <unk> settlers pushed for the road to be improved . This road was part of the Washington – <unk> Revolutionary Route that was used by the French army during their march from Newport , Rhode Island to Yorktown during the Revolutionary War , passing through the area in September <unk> .\n",
      " The road , also known as Old Post Road , was incorporated in 1813 as the Elk and Christiana Turnpike in order to get more money for repairs . The turnpike was completed in April 1817 . As a turnpike , tolls were collected to pay for the maintenance of the road . The construction of the New Castle and Frenchtown Railroad lowered the revenues of the turnpike and it became a public road again in 1838 . The road historically went through agricultural areas ; however , the surroundings have become more developed over the years . Much of the Old Baltimore Pike remains two lanes .\n",
      "\n",
      " = = Major intersections = =\n",
      "\n",
      " The entire route is in New Castle County .\n",
      " <eoa> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(open('wikitext-2/train.txt').readlines()[13].replace(' <eol>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nWLoaJpPAlWG"
   },
   "source": [
    "In order to train the model, we need to feed the data into it. However, the shape of the input data in each iteration and how the model's input affects its performance still remain challenging. One method is to divide the dataset into its sentences and train the model on those sentences. The problem with this method is that it would never allow the model to learn inter-sentence dependencies. On the other hand, one might use paragraphs as the model's inputs. But, since our dataset is a document-level one, it would be wasteful to ignore paragraph-level relations. To sidestep these issues, we view the dataset as a single and very long string and let the model figure out all associations, such as the definition of sentences and paragraphs. In the following steps,  we will implement this method in an efficient data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2010,
     "status": "ok",
     "timestamp": 1588453696813,
     "user": {
      "displayName": "sadra barikbein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCq0YqNHWfcZyly3I_AMTkvS8_tO8PZOQpGtkK=s64",
      "userId": "07513140462530453530"
     },
     "user_tz": -270
    },
    "id": "tG88P6Ns1pG_",
    "outputId": "87ed8125-5814-4aac-b0fe-ed05772a49f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import tempfile\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from torch.nn import LSTM\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "from  torch.distributions.categorical import Categorical\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w_2oLWXk8pQg"
   },
   "outputs": [],
   "source": [
    "# Input pipeline parameters\n",
    "BATCH_SIZE = 80\n",
    "BASE_WINDOW_SIZE = 70\n",
    "VAR_BPTT_STD = 5\n",
    "VAR_BPTT_PROB = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCzeoSDBcUBi"
   },
   "outputs": [],
   "source": [
    "class LMDataset(IterableDataset):\n",
    "  def __init__(self, split_path: str, word2id: Dict[str, int],  \n",
    "               shuffle=False, batch_size=BATCH_SIZE, base_window_size=BASE_WINDOW_SIZE,\n",
    "               variational_bptt_window=False, variational_bptt_std=VAR_BPTT_STD,\n",
    "               variational_bptt_prob=VAR_BPTT_PROB):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "      split_path: path to the dataset file\n",
    "      shuffle: whether to shuffle the dataset\n",
    "      batch_size: the batch size\n",
    "      id2word, word2id: dataset's vocabulary\n",
    "      variational_bptt_window: \n",
    "    \"\"\"\n",
    "    super(LMDataset).__init__()\n",
    "\n",
    "    self.split_path = split_path\n",
    "    self.batch_size = batch_size\n",
    "    self.shuffle = shuffle\n",
    "    self.word2id = word2id\n",
    "    self.base_window_size = base_window_size\n",
    "    self.variational_bptt_window = variational_bptt_window\n",
    "    self.variational_bptt_std = variational_bptt_std\n",
    "    self.variational_bptt_prob = variational_bptt_prob\n",
    "\n",
    "    ### Optinal: you can put additinal codes below here ###\n",
    "    \n",
    "\n",
    "  def _initialize(self):\n",
    "    \"\"\"\n",
    "    See section 1.1.1\n",
    "\n",
    "    This method initalizes the dataset by reading \n",
    "    the file from disk and applying minimum pre-processing\n",
    "    \"\"\"\n",
    "    # A list of word ids\n",
    "    token_ids = []\n",
    "\n",
    "    with open(self.split_path) as data_file:\n",
    "      data=data_file.read().split('\\n')\n",
    "      if self.shuffle:\n",
    "        random.shuffle(data)\n",
    "      tokens='\\n'.join(data).split()\n",
    "      token_ids=list(map(lambda tok:self.word2id[tok.lower()] if (tok.lower() in self.word2id) else self.word2id['<unk>'],tokens))\n",
    "\n",
    "    self.token_ids = torch.tensor(token_ids, dtype=torch.int64)\n",
    "\n",
    "  def __next__(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    See section 1.1.2 and 1.1.3\n",
    "\n",
    "    Returns the next batch of data.\n",
    "\n",
    "    Returns:\n",
    "      x: a 2d (bs x window_size) tensor containing the input word ids\n",
    "      y: a 2d (bs x window_size) tensor, similar to x's shape, \n",
    "          containing target word ids\n",
    "\n",
    "    Dont forget to use self.batch_size as the batch size\n",
    "      and self.base_seq_len as the window size \n",
    "    \"\"\"\n",
    "    x=[]\n",
    "    y=[]\n",
    "    row_length=int(len(self.token_ids)/self.batch_size)\n",
    "    data=self.token_ids\n",
    "    window=self.base_window_size if not self.variational_bptt_window else max(1,int(random.gauss(self.base_window_size*0.5**np.random.binomial(1,self.variational_bptt_prob),self.variational_bptt_std)))\n",
    "    end_x=self.curr_batch*self.base_window_size if not self.variational_bptt_window else self.last_x+window\n",
    "    if end_x>=row_length:\n",
    "      raise StopIteration()\n",
    "    length=min(window,row_length-end_x-1)\n",
    "    for i in range(self.batch_size):\n",
    "      start=i*row_length+end_x\n",
    "      x+=[list(data[start:start+length])]\n",
    "      y+=[list(data[start+1:start+1+length])]\n",
    "      if len(x[-1])==0 or len(y[-1])==0:\n",
    "        raise StopIteration()\n",
    "    self.curr_batch+=1\n",
    "    self.last_x=end_x\n",
    "    return torch.tensor(x,dtype=torch.long), torch.tensor(y,dtype=torch.long)\n",
    "\n",
    "  def __iter__(self):\n",
    "    \"\"\"\n",
    "    This method get called only at begining of the iteration\n",
    "    \"\"\"\n",
    "    self._initialize()\n",
    "    self.curr_batch=0\n",
    "    self.last_x=0\n",
    "    ### Optinal: you can put additinal codes below here ###\n",
    "\n",
    "    return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1G_8mM60B9A"
   },
   "source": [
    "#### 1.1.1 Read and Tokenize (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9pNLTufp6SsH"
   },
   "source": [
    "First, we need to load the dataset file and apply some preprocessing on it. Specifically, you need to: \n",
    "1. Read the file from the disk.\n",
    "2. Shuffle articles if required (please note that every line in the dataset represents a single Wikipedia article).\n",
    "3. Concatenate all articles into one big string.\n",
    "4. Tokenize it (since the dataset is already tokenized, you can get all tokens by simply splitting the big string by whitespaces).\n",
    "5. Convert all tokens into their corresponding ids (the unknown token is `<unk>`. Make sure that you will use it for out-of-vocabulary tokens)\n",
    "\n",
    "*Note: Run the following cell before implementing this section*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1839,
     "status": "ok",
     "timestamp": 1588448620748,
     "user": {
      "displayName": "sadra barikbein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCq0YqNHWfcZyly3I_AMTkvS8_tO8PZOQpGtkK=s64",
      "userId": "07513140462530453530"
     },
     "user_tz": -270
    },
    "id": "AXeirRsz1lP3",
    "outputId": "50e09374-3f6c-46dd-cd9d-c35c4b306966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2ids: [('the', 0), (',', 1), ('.', 2), ('<unk>', 3), ('of', 4), ('and', 5), ('in', 6), ('to', 7), ('a', 8), ('<eol>', 9)]\n"
     ]
    }
   ],
   "source": [
    "vocab = pickle.load(open('wikitext-2/vocab.pk', 'rb'))\n",
    "word2id, id2word = vocab['word2id'], vocab['id2word']\n",
    "print(\"word2ids:\", list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zxn89qBI8LuN"
   },
   "source": [
    "Check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2177,
     "status": "ok",
     "timestamp": 1588455346641,
     "user": {
      "displayName": "sadra barikbein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCq0YqNHWfcZyly3I_AMTkvS8_tO8PZOQpGtkK=s64",
      "userId": "07513140462530453530"
     },
     "user_tz": -270
    },
    "id": "8IGKu5zr17-F",
    "outputId": "ebfab2f6-2bf7-4fd2-de14-2133493f95ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "train_ds = LMDataset('wikitext-2/train.txt', shuffle=True, word2id=word2id)\n",
    "it = iter(train_ds)\n",
    "\n",
    "assert train_ds.token_ids.shape[0] == 2130493\n",
    "assert train_ds.token_ids.sum() == 3692975507\n",
    "print(\"Passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WfOvA3wt4nVS"
   },
   "source": [
    "#### 1.1.2 Create batches (6 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VGMWVxo74nLQ"
   },
   "source": [
    "\n",
    "Up to this point, we only have a single list of tokens, which basically contains the whole dataset. Training a recurrent network on such a big sequence is practically impossible since there is no way that we can backprop through millions of time-steps. Moreover, the GPU's memory is minimal, so that it will exhaust on the first few hundred steps. Therefore, we have no choice other than splitting the big string into smaller chunks. Obviously, by chunking the dataset into equal-sized sequences, the sentences or paragraphs might get cut in the middle. However, we should accept this tradeoff in order to train the model.\n",
    "\n",
    "Let's start with a straightforward chunking-technique. Imagine that we have a 40-token string; we divide it into eight 5-token substrings, and the batch size is 4:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://imgur.com/download/6HjGVpG\"/>\n",
    "</p>\n",
    "\n",
    "In each iteration, the RNN is computed independently for every row of the batch. Take the first batch as an example: The RNN for the second row starts from the token $t_6$, which can be in the middle of the original sequence. Such restriction for such a short and close dependency can significantly hurt the model's performance.\n",
    "\n",
    "Now imagine another approach to this issue. Take the original sequence of 40 elements. Divide it into 4 (the batch size) long sequences.\n",
    "<p align=\"center\">\n",
    "<img src=\"https://imgur.com/download/NXLfuaI\"/>\n",
    "</p>\n",
    "Now, given a backpropagation window size of 4, we can create the batches as following:\n",
    "<p align=\"center\">\n",
    "<img src=\"https://imgur.com/download/RFFyZHD\"/>\n",
    "</p>\n",
    "\n",
    "So let's discuss why this technique is actually better. First of all, dividing the big string by batch size (which in here is 4) produces very long rows. Each row may contain several articles (as opposed to the previous method in which each row merely contains few sentences). Therefore, most of the contents between any two consecutive rows are independent. Secondly, creating batches by moving a window across the horizontal axis enables us to use the hidden states computed from the previous batch. For instance, in the figure above, we can initialize the RNN's hidden states of the second batch with the final states of batch one. Although the gradients do not flow from the second batch to the first one, the RNN can have access to some information from the previous chunk. \n",
    "\n",
    "In this section, we are going to implement the last chunking method. Specifically, you should complete the implementation of the `LMDataset.__next__()` method. In the future, this function will be used to iterate through the dataset. Therefore, on each call, it should return the next model's inputs and ground truth labels (Note that labels are the shifted inputs in the language modeling task). To implement the method, first, divide the tokens_ids by the batch size (to have equally-sized subsequences, you should drop the remainder). Then, generate batches according to the algorithm mentioned above. Please note that the final batches may not have the same window size. Also, when there are no more batches to generate, raise a `StopIteration` exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w4Rsgz-5zkW3"
   },
   "source": [
    "Check your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1129,
     "status": "ok",
     "timestamp": 1588455352538,
     "user": {
      "displayName": "sadra barikbein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCq0YqNHWfcZyly3I_AMTkvS8_tO8PZOQpGtkK=s64",
      "userId": "07513140462530453530"
     },
     "user_tz": -270
    },
    "id": "fd6l24KFo5Fj",
    "outputId": "d2a7ed2b-30bb-4d3b-8a75-afa55aae7ae7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy dataset\n",
    "with tempfile.NamedTemporaryFile('w', delete=False) as f:\n",
    "  dummy_ds = ' '.join([str(i) for i in range(1, 41)]) + '\\n'\n",
    "  f.write(dummy_ds)\n",
    "  \n",
    "dummy_word2id = {str(i): i for i in range(1, 41)}\n",
    "dummy_word2id.update({'<unk>': 1})\n",
    "dummy_ds = LMDataset(f.name, shuffle=True, word2id=dummy_word2id,\n",
    "                     batch_size=4, base_window_size=5)\n",
    "xys = list(dummy_ds)\n",
    "assert len(xys) == 2\n",
    "assert torch.all(xys[0][0] == torch.tensor([[ 1,  2,  3,  4,  5],\n",
    "        [11, 12, 13, 14, 15],\n",
    "        [21, 22, 23, 24, 25],\n",
    "        [31, 32, 33, 34, 35]]))\n",
    "assert torch.all(xys[0][1] == torch.tensor([[ 2,  3,  4,  5,  6],\n",
    "          [12, 13, 14, 15, 16],\n",
    "          [22, 23, 24, 25, 26],\n",
    "          [32, 33, 34, 35, 36]]))\n",
    "assert torch.all(xys[1][0] == torch.tensor([[ 6,  7,  8,  9],\n",
    "          [16, 17, 18, 19],\n",
    "          [26, 27, 28, 29],\n",
    "          [36, 37, 38, 39]]))\n",
    "assert torch.all(xys[1][1] == torch.tensor([[ 7,  8,  9, 10],\n",
    "          [17, 18, 19, 20],\n",
    "          [27, 28, 29, 30],\n",
    "          [37, 38, 39, 40]]))\n",
    "\n",
    "print('Passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "531k4naKVEjL"
   },
   "source": [
    "#### 1.1.3 Variable backpropagation window size (3.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWePD23jYyi-"
   },
   "source": [
    "Although we have managed to make the batching as efficient as possible and made use of final hidden states from the previous training iteration as an initialization for the current batch, some of the elements still receive no backpropagation window at all.  \n",
    "\n",
    "Given a window size of 5, how many dataset elements receive no backpropagation windows? (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QKSf1NZQjdNQ"
   },
   "source": [
    "In the example above ,last column items meaning 4 items do not get backprop window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NPMxLbyijsRo"
   },
   "source": [
    "To remedy this problem, we can use a variable window size that is randomly sampled. Such a procedure will prevent those elements from being at the same position within the backprop window among different epochs. Therefore, when given enough training epochs, we will anticipate that every element receives an adequate backpropagation window. Specifically, we use the following formula to calculate the window size for every batch:\n",
    "\n",
    "$$\n",
    "\\text{ } \\\\\n",
    "\\hat{w} = \\left\\{ \\begin{array}{rl}\n",
    "\\mathcal{W} &\\mbox{with probability} \\ p \\\\\n",
    "\\frac{1}{2} . \\mathcal{W} &\\mbox{with probability} 1-p\n",
    "\\end{array} \\right.\n",
    "\\\\ \\text{ }\n",
    "\\\\ \\text{ }\n",
    "w \\sim \\mathcal{N}(\\hat{w}, \\sigma)\n",
    "$$\n",
    "\n",
    "where $\\mathcal{W}$ is the default window size, $p$ is the a probability close to 1, $\\sigma$ is the standard deviation, and $w$ is the final window size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QmBf4eJjsIu"
   },
   "source": [
    "In this section, you should edit the `__next__(self)` method to incoporate a variable window size. Since we will the same class for our validation data, the variable mechanism should be only activated when `self.variational_bptt_window` flag is true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-XvoqNcvPJh"
   },
   "source": [
    "Check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24303,
     "status": "ok",
     "timestamp": 1588455380384,
     "user": {
      "displayName": "sadra barikbein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCq0YqNHWfcZyly3I_AMTkvS8_tO8PZOQpGtkK=s64",
      "userId": "07513140462530453530"
     },
     "user_tz": -270
    },
    "id": "Wdk4CVgCrAZz",
    "outputId": "fd5f9ca2-9700-4d19-bd08-c55c103242fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "xys = list(LMDataset('wikitext-2/train.txt', shuffle=True, word2id=word2id, \n",
    "                    variational_bptt_window=True))[:100]\n",
    "assert np.mean([x.shape[1] for x, y in xys]) != BASE_WINDOW_SIZE\n",
    "\n",
    "xys = list(LMDataset('wikitext-2/train.txt', shuffle=True, word2id=word2id, \n",
    "                    variational_bptt_window=False))[:100]\n",
    "assert np.mean([x.shape[1] for x, y in xys]) == BASE_WINDOW_SIZE\n",
    "print('Passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kobosW-_yUkQ"
   },
   "source": [
    "### 1.2 Implementing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pDC8IwgJStsd"
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://imgur.com/download/LeK6lQh\"/>\n",
    "</p>\n",
    "\n",
    "For this assignment, we will use a simple stacked LSTM architecture and various regularized techniques to improve the model's performance and overfitting. Our primary regularization method is Dropout. But, vanilla Dropout is not quite suitable for RNNs. So, as we will explain in the following sections, some modifications are required. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d-PZvzAzYQcP"
   },
   "source": [
    "#### 1.2.1 ConnectionDrop LSTM (Optional: 6 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8jADAEnYY7e"
   },
   "source": [
    "The problem with the standard Dropout in the context of RNNs is that it generates a new binary mask every time it gets called. Howerver, such behavior is problematic in RNNs. If we use the original Dropout on the hidden-to-hidden connection, then the RNN cell will receive a completely different mask for every timestep (in a non-recurrent layer, the Dropout mask for its inputs remains consistent through forward and backward flow). One of the methods to solve that issue is applying Dropout on the layer/cell's weight (instead of activation), which is similar to removing connections between two consecutive layers. As this Dropout only gets called once per each training step, the repeated computation of an RNN cell would not be a problem.\n",
    "\n",
    "In this section, we would like to extend the vanilla LSTM layer so that it applies weight dropout on its hidden-to-hidden connection. The corresponding weight for the mentioned connection is called `weight_hh_l0`. Implement this functionality in the below class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IBZ7tlBTjQZI"
   },
   "outputs": [],
   "source": [
    "class ConnectionDropLSTM(LSTM):\n",
    "  def __init__(self, *args, dropout_rate=0.0, **kwargs):\n",
    "    super(ConnectionDropLSTM, self).__init__(*args, **kwargs)\n",
    "    self.dropout_rate = dropout_rate\n",
    "    self.is_renamed = False\n",
    "\n",
    "  def flatten_parameters(self, *arg, **kwargs):\n",
    "    # Do not change this. This is a temp fix for a wierd \n",
    "    # issue with cudnn LSTM\n",
    "    return\n",
    "\n",
    "  def _rename_orig_param(self):\n",
    "    # 1- Register parameter `weight_hh_l0` as a new parameter called `weight_hh_l0_orig`\n",
    "    # 2- Remove `weight_hh_l0` from the model's parameters\n",
    "    # hint: see the documentation for torch.nn.Module.register_parameter and\n",
    "    #          torch.nn.Module._parameters\n",
    "\n",
    "    self.register_parameter('weight_hh_l0_orig',self.weight_hh_l0)\n",
    "    del self.weight_hh_l0\n",
    "    self.is_renamed = True\n",
    "\n",
    "  def _drop_connections(self):\n",
    "    \"\"\"\n",
    "    Applies dropout on hidden-to-hidden paramters\n",
    "    \"\"\"\n",
    "\n",
    "    # 1- Create a new tensor by applying dropout on `weight_hh_l0_orig`\n",
    "    # 2- Assing the newly created tensor to instance attribute `weight_hh_l0`\n",
    "\n",
    "    self.weight_hh_l0=F.dropout(self.weight_hh_l0_orig,training=self.training,p=self.dropout_rate)\n",
    "\n",
    "  def forward(self, *args, **kwargs):\n",
    "    if not self.is_renamed: \n",
    "      self._rename_orig_param()\n",
    "    \n",
    "    self._drop_connections()\n",
    "    return super().forward(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3IHQpouFyE1h"
   },
   "source": [
    "Check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1588342795899,
     "user": {
      "displayName": "sadra barikbein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCq0YqNHWfcZyly3I_AMTkvS8_tO8PZOQpGtkK=s64",
      "userId": "07513140462530453530"
     },
     "user_tz": -270
    },
    "id": "BObXrExoxox6",
    "outputId": "f8fa8da3-b4a2-4f18-cfc0-a079e3620c45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "cell = ConnectionDropLSTM(20, 20, dropout_rate=0.1)\n",
    "\n",
    "x = torch.ones(3, 1, 20)\n",
    "out1, _ = cell(x)\n",
    "\n",
    "cell_params = list(cell.named_parameters())\n",
    "\n",
    "assert len(cell_params) == 4\n",
    "assert 'weight_hh_l0_orig' in [n for n,_ in cell_params]\n",
    "assert 'weight_hh_l0' not in [n for n,_ in cell_params]\n",
    "assert isinstance(cell.weight_hh_l0_orig, torch.nn.Parameter)\n",
    "assert hasattr(cell, 'weight_hh_l0')\n",
    "assert not isinstance(cell.weight_hh_l0, torch.nn.Parameter) and \\\n",
    "       cell.weight_hh_l0.requires_grad\n",
    "\n",
    "out2, _ = cell(x)\n",
    "\n",
    "assert torch.all(out2[0] == out1[0])\n",
    "assert torch.all(out2[1] != out1[1])\n",
    "assert torch.all(out2[2] != out1[2])\n",
    "\n",
    "print('Passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aPCwAR9-D-rT"
   },
   "source": [
    "#### 1.2.2 Locked Dropout (1.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wrRMKx-PJR-x"
   },
   "source": [
    "Other than hidden-to-hidden connections, an RNN cell has two another  connections: input and output. Fortunately, we will use a much simpler solution to sidestep the aforementioned problem. Locked Dropout is another version of Dropout that generates the mask only once per iterations and then applies the mask to all corresponding connections within the timesteps. For more information, see figure 2 and pay attention to symbols on each \"Locked Dropout\" block. Similar symbols mean that they use the same mask on their corresponding connections. Implement this method in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nm3WFXoWJS7v"
   },
   "outputs": [],
   "source": [
    "def locked_dropout(activations: torch.Tensor, dropout_rate=0.1, training=True):\n",
    "  \"\"\"\n",
    "  Applies the same binary mask across the sequence length axis\n",
    "\n",
    "  Args:\n",
    "    activations (torch.Tensor(shape=[batch, seq_len, x])): A tensor that\n",
    "        we applies the dropout on\n",
    "    drouput_rate (float): Dropout rate (0 = no dropout, 1 = zero out all units)\n",
    "    training (bool): whether we are at the training phase\n",
    "\n",
    "  Returns:\n",
    "    torch.Tensor(shape=[batch, seq_len, x]): activations after applying the mask\n",
    "\n",
    "  Hint: Do not use PyTorch's built-in dropout function. You should implement it \n",
    "      yourself. Be careful about the differences between training and inference\n",
    "      phases.\n",
    "  \"\"\"\n",
    "  # 1- Create a unique binary mask for each element within the batch\n",
    "  # 2- Repead the masks across the sequence length\n",
    "  # 3- Compute the scalar product between the mask and activations\n",
    "\n",
    "  mask = torch.ones_like(activations, dtype=torch.float,device=DEVICE)\n",
    "\n",
    "  if training:\n",
    "    mask=torch.tensor(np.random.binomial(1,1.-dropout_rate,size=(mask.size(0),1,mask.size(2)))/(1.-dropout_rate),device=DEVICE,dtype=torch.float).repeat(1,mask.size(1),1)\n",
    "\n",
    "  return mask * activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t_7_pNBANuLr"
   },
   "source": [
    "Check you implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9432,
     "status": "ok",
     "timestamp": 1588415981023,
     "user": {
      "displayName": "sadra barikbein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCq0YqNHWfcZyly3I_AMTkvS8_tO8PZOQpGtkK=s64",
      "userId": "07513140462530453530"
     },
     "user_tz": -270
    },
    "id": "79W_coPhNtwm",
    "outputId": "5be08053-e1a0-4270-919c-1563f08348d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((100, 3, 4), dtype=torch.float,device=DEVICE)\n",
    "x_drop = locked_dropout(x, dropout_rate=0.4)\n",
    "\n",
    "assert x.shape == x_drop.shape\n",
    "assert torch.abs(x_drop.sum() - x.sum()) < 100\n",
    "assert torch.all(x_drop[:, 0, :] == x_drop[:, 1, :])\n",
    "x_drop2 = locked_dropout(x, dropout_rate=0.4)\n",
    "assert torch.any(x_drop != x_drop2)\n",
    "print('Passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iI5Xujfqe_H6"
   },
   "source": [
    "#### 1.2.3 Embedding + Masking (1.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c9owYA3xn1Ke"
   },
   "source": [
    "One of the applications of Dropout in standard Neural Networks is putting noise on the input vector. Applying Dropout on a single d-dim input vector might zero out some of its dimensions. But, using the exact same Dropout technique for the RNNs input, i.e., sentences, barely masks some units in some words' vector, while the base unit in RNNs input is a word. Moreover, several occurrences of the same word may receive different masking. To mitigate the problem, we mask out a word from embedding entirely. That is, if we would like to drop a word in the input layer of the network, we will change the whole row corresponding to that word in the embedding matrix to zero. See more info in [1]\n",
    "\n",
    "In the following function, you should apply this technique to the weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WINOK5PuoCZc"
   },
   "outputs": [],
   "source": [
    "def mask_embedding(embed_weight: torch.Tensor, dropout_rate=0.1, training=True):\n",
    "  \"\"\"\n",
    "  Applies Dropout at word-level\n",
    "\n",
    "  Args:\n",
    "    embed_weight (torch.Tensor(shape=[vocab_size, embed_dim]): the embedding matrix\n",
    "    dropout_rate (float): dropout rate\n",
    "    training (bool): whether we are at the training phase\n",
    "\n",
    "  Returns:\n",
    "    torch.Tensor(shape=[vocab_size, embed_dim]): Masked embeddingg matrix\n",
    "\n",
    "  Hint: Do not use PyTorch built-in dropout function. You should implement it \n",
    "      yourself. Be careful about the differences between training and inference\n",
    "      phases.\n",
    "  \"\"\"\n",
    "  if training:\n",
    "    embed_weight*= torch.tensor(np.random.binomial(1,1.-dropout_rate,size=(embed_weight.size(0),))/(1.-dropout_rate),device=DEVICE).reshape(-1,1)\n",
    "\n",
    "\n",
    "  return embed_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c32Wbu_aqgmL"
   },
   "source": [
    "Check your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1063,
     "status": "ok",
     "timestamp": 1588372385546,
     "user": {
      "displayName": "sadra barikbein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCq0YqNHWfcZyly3I_AMTkvS8_tO8PZOQpGtkK=s64",
      "userId": "07513140462530453530"
     },
     "user_tz": -270
    },
    "id": "dx22jgtyqjhw",
    "outputId": "dbda3455-eb8e-4dbd-ce43-a24b784c40b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non masked:\n",
      "tensor([[1.4286, 1.4286, 1.4286, 1.4286],\n",
      "        [1.4286, 1.4286, 1.4286, 1.4286],\n",
      "        [1.4286, 1.4286, 1.4286, 1.4286],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.4286, 1.4286, 1.4286, 1.4286],\n",
      "        [1.4286, 1.4286, 1.4286, 1.4286],\n",
      "        [1.4286, 1.4286, 1.4286, 1.4286],\n",
      "        [1.4286, 1.4286, 1.4286, 1.4286],\n",
      "        [1.4286, 1.4286, 1.4286, 1.4286]], device='cuda:0')\n",
      "masked:\n",
      "tensor([[1.4286, 1.4286, 1.4286, 1.4286],\n",
      "        [1.4286, 1.4286, 1.4286, 1.4286],\n",
      "        [1.4286, 1.4286, 1.4286, 1.4286],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.4286, 1.4286, 1.4286, 1.4286],\n",
      "        [1.4286, 1.4286, 1.4286, 1.4286],\n",
      "        [1.4286, 1.4286, 1.4286, 1.4286],\n",
      "        [1.4286, 1.4286, 1.4286, 1.4286],\n",
      "        [1.4286, 1.4286, 1.4286, 1.4286]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "embed_weight = torch.ones((10, 4), dtype=torch.float,device=DEVICE)\n",
    "masked_weight = mask_embedding(embed_weight, dropout_rate=0.3)\n",
    "\n",
    "assert masked_weight.shape == embed_weight.shape\n",
    "\n",
    "print(\"non masked:\")\n",
    "print(embed_weight)\n",
    "print(\"masked:\")\n",
    "print(masked_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hq9_cs03sitZ"
   },
   "source": [
    "#### 1.2.4 Weight tying (1.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OK-reRwL0PYR"
   },
   "source": [
    "Weight tying ties the embedding matrix to the final softmax layer's weights (vocab projection), i.e., these two layers will be forced to share the same set of parameters. Such sharing reduces the number of model parameters and prevents the model from learning a one-to-one relation between the input weights and the output weights. For a theoretical justification of this work, please refer to [2].\n",
    "\n",
    "*Note that you will implement this in section 1.2.6*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6Qd-pk50Qed"
   },
   "source": [
    "#### 1.2.5 Activity Regularization and Temporal Activation Regularization (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2YfFjma-03_S"
   },
   "source": [
    "**Activity Regularization** penalizes the model on having a hidden state with very large values, effectively making it have more stable hidden states. Here is the formulation:\n",
    "$$\n",
    "\\alpha L_2(m \\circ h_t)\n",
    "$$\n",
    "where $L_2(x) = \\Vert x \\Vert$, and $m \\circ h_t$ is the dropped hidden state for the last layer (AR only applies to the last RNN layer).\n",
    "\n",
    "**Temporal Activation Regularization** penalizes the model on having a large or sudden change between two consecutive hidden states, which helps the model to have more consistent hidden states. Here is the formulation:\n",
    "$$\n",
    "\\beta \\ . L_2(h_t - h_{t-1})\n",
    "$$\n",
    "Same as the AR, TAR only applies to the last RNN layer.\n",
    "Find more info in [3]\n",
    "\n",
    "*Note that you will implement this in section 1.2.6*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BeSDDzLB6KdO"
   },
   "source": [
    "#### 1.2.6 The model (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KUaLAI5O6PHF"
   },
   "source": [
    "After implementing different components of our extended LSTM language model, now it's time to put them all together. Use the functions and classes that you have implemented in the previous sections to fill the model's class below. Your model should have the following features:\n",
    "1. ConnectionDrop LSTM (Optional): Use this class instead of vanilla LSTM\n",
    "2. Use the `locked_dropout` at appropriate places in the model (You may want to refer to figure 2)\n",
    "3. Apply the masked dropout on the embedding matrix.\n",
    "4. Tie the weight of the embedding layer and the final softmax layer (Note that you should tie the non-masked version of the embedding matrix).\n",
    "5. You should also implement the AR and TAR, as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hj2bL6PtdPJE"
   },
   "outputs": [],
   "source": [
    "# Model Hyper parameters\n",
    "NUM_LSTM_LAYER = 3\n",
    "HIDDEN_SIZE = 250#1150\n",
    "EMBED_DIM = 400\n",
    "DR_EMBED = 0.1\n",
    "DR_INPUT = 0.65\n",
    "DR_HIDDEN = 0.2\n",
    "DR_OUTPUT = 0.4\n",
    "CONN_DR = 0.5\n",
    "AR_COEF = 2\n",
    "TAR_COEF = 1\n",
    "VOCAB_SIZE = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S__RgI1Y9Gmv"
   },
   "outputs": [],
   "source": [
    "class ExtendedLM(nn.Module):\n",
    "  def __init__(self, num_lstm_layers=NUM_LSTM_LAYER, hidden_size=HIDDEN_SIZE, \n",
    "               vocab_size=VOCAB_SIZE, embed_dim=EMBED_DIM, \n",
    "               drop_rate_embed=DR_EMBED, drop_rate_input=DR_INPUT, \n",
    "               drop_rate_hidden=DR_HIDDEN, drop_rate_output=DR_OUTPUT, \n",
    "               lstm_conndrop_rate=CONN_DR, ar_coeff=AR_COEF, tar_coeff=TAR_COEF):\n",
    "    super(ExtendedLM, self).__init__()\n",
    "\n",
    "    self.ar_coeff=ar_coeff\n",
    "    self.tar_coeff=tar_coeff\n",
    "    self.drop_rate_embed=drop_rate_embed\n",
    "    self.drop_rate_input=drop_rate_input\n",
    "    self.drop_rate_output=drop_rate_output\n",
    "    self.drop_rate_hidden=drop_rate_hidden\n",
    "    self.num_lstm_layers=num_lstm_layers\n",
    "    cells=[ConnectionDropLSTM(embed_dim if i==0 else hidden_size,embed_dim if i+1==num_lstm_layers else hidden_size,dropout_rate=lstm_conndrop_rate) for i in range(num_lstm_layers)]\n",
    "    self.LSTMcells=nn.ModuleList(cells)\n",
    "    self.toVocab=nn.Linear(embed_dim,vocab_size)\n",
    "    \n",
    "\n",
    "\n",
    "  def init_zero_state(self, batch_size):\n",
    "    \"\"\"\n",
    "    Returns the initial state (h_0, c_0) for LSTM networks.\n",
    "    Both h_0 and c_0 are created by non-trainable \n",
    "    tensors containing zeros\n",
    "\n",
    "    Arg:\n",
    "      batch_size (int): batch size for the input data\n",
    "\n",
    "    Returns:\n",
    "      List[Tuple(\n",
    "        torch.Tensor(shape=[1, batch_size, hidden_size]),\n",
    "        torch.Tensor(shape=[1, batch_size, hidden_size])\n",
    "      )]\n",
    "\n",
    "      it returns one tuple per each LSTM layer\n",
    "    \"\"\"\n",
    "\n",
    "    return [(torch.zeros(1,batch_size,cell.hidden_size),torch.zeros(1,batch_size,cell.hidden_size)) for cell in self.LSTMcells]\n",
    "\n",
    "  def forward(self, inputs, initial_hiddens):\n",
    "    \"\"\"\n",
    "    Forward pass of the model\n",
    "\n",
    "    Args:\n",
    "      inputs (torch.Tensor(shape=[batch_size, seq_len], dtype=torch.int)):\n",
    "          Input of the model, which is a 2D tensor containing input word ids\n",
    "      initial_hidden (List[Tuple(\n",
    "                        torch.Tensor(shape=[1, batch_size, hidden_size]),\n",
    "                        torch.Tensor(shape=[1, batch_size, hidden_size])\n",
    "                      )]): the initial state for the LSTM modules. On the first call\n",
    "                            they are all zero. but, all other next batches are just \n",
    "                            previous state computed during the previous batch\n",
    "    Returns:\n",
    "      predictions (torch.Tensor(batch_size, seq_len, vocab_size)): the output of the model\n",
    "      final_hidden (List[Tuple(\n",
    "                        torch.Tensor(shape=[1, batch_size, hidden_size]),\n",
    "                        torch.Tensor(shape=[1, batch_size, hidden_size])\n",
    "                      )]): computed hidden state from the last step of LSTMs\n",
    "      regularization_losses torch.Tensor(shape=[], dtype=float): Accumulated for \n",
    "          regularization methods. \n",
    "    \"\"\"\n",
    "    final_hidden=[]\n",
    "    embed_weight=self.toVocab.weight.clone()\n",
    "    mask_embedding(embed_weight,dropout_rate=self.drop_rate_embed,training=self.training)\n",
    "    droppedEmbedded=F.embedding(inputs,embed_weight)\n",
    "    x=locked_dropout(droppedEmbedded, dropout_rate=self.drop_rate_input, training=self.training).permute(1,0,2)\n",
    "    for i,cell in enumerate(self.LSTMcells):\n",
    "      #print('x:',x.shape,'ini_h:',initial_hiddens[i])\n",
    "      x,final=cell(x,initial_hiddens[i])\n",
    "      final_hidden+=[final]\n",
    "      x=locked_dropout(x, dropout_rate=self.drop_rate_output if i+1==self.num_lstm_layers else self.drop_rate_hidden, training=self.training)\n",
    "      if i+1==self.num_lstm_layers:\n",
    "        AR_loss=self.ar_coeff*torch.norm(torch.mean(x,dim=[0,1]))\n",
    "        TAR_loss=0.\n",
    "        for seq in range(x.size(0)):\n",
    "          if seq==0:\n",
    "            TAR_loss+=self.tar_coeff*torch.norm(torch.mean(x[seq]-initial_hiddens[i][0][0],dim=0))\n",
    "          else:\n",
    "            TAR_loss+=self.tar_coeff*torch.norm(torch.mean(x[seq]-x[seq-1],dim=0))\n",
    "        regularization_losses=AR_loss+TAR_loss\n",
    "    predictions=self.toVocab(x).permute(1,0,2)\n",
    "    \n",
    "\n",
    "    return predictions,final_hidden,regularization_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dv0NYG-jQm6B"
   },
   "source": [
    "### 1.3 Training (6 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rMgG6KVCQu0W"
   },
   "source": [
    "Now it is time to implement the train-and-eval loop for our model. You should use the LMDataset class that you have implemented in section 1.1 . In addition, please pay attention to these notes:\n",
    "\n",
    "1. Remember to detatch the previous hiddens states from the computational graph in every training itereration. Otherwise, the model will backpropagate all through the begining of dataset\n",
    "2. Use gradient clipping\n",
    "3. Don't forget to use the GPU. Enable it using `model = model.cuda()`\n",
    "4. Remember to add the regularization terms to the final loss\n",
    "5. Use `Adam` as your optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m5cGtnbic1iC"
   },
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "NUM_EPOCH = 40\n",
    "SEED = 1543\n",
    "EVAL_INTERVAL = 500\n",
    "GRAD_CLIP = 0.25\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CpTuVNu_RbSq"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, ds):\n",
    "  \"\"\"\n",
    "  Evaluate the model on the validation set, and return loss and perplexity\n",
    "\n",
    "  Args:\n",
    "    model (ExtendedLM): An instance of the model\n",
    "    ds (LMDataset): The target dataset to calculate the performance\n",
    "\n",
    "  Returns:\n",
    "    loss (float): Average loss\n",
    "    ppl (float): Average perplexity\n",
    "\n",
    "  Hint: Do not forget to put the model in the evaluation mode\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "  hiddens=list(map(lambda hc:(hc[0].to(DEVICE),hc[1].to(DEVICE)),model.init_zero_state(BATCH_SIZE)))\n",
    "  losses=[]\n",
    "  entropySum=0.\n",
    "  entropyCnt=0\n",
    "  for x,y in ds:\n",
    "    x,y=x.to(DEVICE),y.to(DEVICE)\n",
    "    predictions,hiddens,regularization_losses=model(x,hiddens)\n",
    "    entropySum+=torch.sum(Categorical(predictions[:,-1,:]).entropy()).item()\n",
    "    entropyCnt+=predictions.size(0)\n",
    "    hiddens=list(map(lambda hc:(hc[0].detach(),hc[1].detach()),hiddens))\n",
    "    loss=F.nll_loss(predictions.reshape(predictions.size(0)*predictions.size(1),predictions.size(2)),y.view(y.size(0)*y.size(1))).item()\n",
    "    loss+=regularization_losses.item()\n",
    "    losses+=[loss]\n",
    "  loss=np.mean(losses)\n",
    "  print('Evaluation : Validation loss: {}'.format(loss))\n",
    "  model.train()\n",
    "  return loss,2**(entropySum/float(entropyCnt))\n",
    "\n",
    "def train(model_path='lm_model.pt'):\n",
    "  \"\"\"\n",
    "  Train and save the model on the disk\n",
    "\n",
    "  Args:\n",
    "    model_path (str): Where to save checkpoints\n",
    "\n",
    "  Returns:\n",
    "    loss (float): The training loss\n",
    "    ppl (float): The training perplexity\n",
    "    model (ExtendedLM): The trained model\n",
    "  \"\"\"\n",
    "  # Set the seed to have a reproducable experiments\n",
    "  torch.manual_seed(SEED)\n",
    "  torch.cuda.manual_seed(SEED)\n",
    "  np.random.seed(SEED)\n",
    "\n",
    "  # Load the training & validation dataset\n",
    "  train_ds = LMDataset(\n",
    "      'wikitext-2/train.txt', word2id=word2id, shuffle=True, variational_bptt_window=True)\n",
    "  valid_ds = LMDataset(\n",
    "      'wikitext-2/valid.txt', word2id=word2id, batch_size=10)\n",
    "  model=ExtendedLM()\n",
    "  optimizer=torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "  model=model.to(DEVICE)\n",
    "  tr_loss_epochs=[]\n",
    "  vl_loss_epochs=[]\n",
    "  entropySum=0.\n",
    "  entropyCnt=0\n",
    "  cnt=0\n",
    "  for epoch in range(NUM_EPOCH):\n",
    "    hiddens=list(map(lambda hc:(hc[0].to(DEVICE),hc[1].to(DEVICE)),model.init_zero_state(BATCH_SIZE)))\n",
    "    losses=[]\n",
    "    for x,y in train_ds:\n",
    "      x,y=x.to(DEVICE),y.to(DEVICE)\n",
    "      predictions,hiddens,regularization_losses=model(x,hiddens)\n",
    "      if epoch==NUM_EPOCH-1:\n",
    "        entropySum+=torch.sum(Categorical(F.softmax(predictions[:,-1,:],dim=-1)).entropy()).item()\n",
    "        entropyCnt+=predictions.size(0)\n",
    "      hiddens=list(map(lambda hc:(hc[0].detach(),hc[1].detach()),hiddens))\n",
    "      loss=F.nll_loss(predictions.reshape(predictions.size(0)*predictions.size(1),predictions.size(2)),y.view(y.size(0)*y.size(1)))\n",
    "      loss+=regularization_losses\n",
    "      losses+=[loss.item()]\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      nn.utils.clip_grad_norm_(model.parameters(),GRAD_CLIP)\n",
    "      optimizer.step()\n",
    "      if epoch==0:\n",
    "        print(cnt,loss.item())\n",
    "        cnt+=1\n",
    "    loss=np.mean(losses)\n",
    "    print('Epoch {}: Train Loss: {}'.format(epoch+1,loss))\n",
    "    tr_loss_epochs+=[loss]\n",
    "    if (epoch+1)%EVAL_INTERVAL==0:\n",
    "      loss,ppl=evaluate(model,valid_ds)\n",
    "      print('Evaluation : Validation loss: {}'.format(loss))\n",
    "      vl_loss_epochs+=[loss]\n",
    "  torch.save(model.state_dict(),model_path)\n",
    "  plt.plot(tr_loss_epochs,'-g', label='train')\n",
    "  plt.plot(vl_loss_epochs,'-r', label='validation')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.ylabel('loss')\n",
    "  plt.legend(loc='upper right')\n",
    "  plt.show()\n",
    "  return tr_loss_epochs[-1],2**(entropySum/float(entropyCnt)),model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0fA5GfgEW5S"
   },
   "source": [
    "Mount Google Drive (Optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40772,
     "status": "ok",
     "timestamp": 1588448753754,
     "user": {
      "displayName": "sadra barikbein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCq0YqNHWfcZyly3I_AMTkvS8_tO8PZOQpGtkK=s64",
      "userId": "07513140462530453530"
     },
     "user_tz": -270
    },
    "id": "Ys58M6ZfEV6a",
    "outputId": "0b305ddf-9518-4ce6-82d4-0dc054913f21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /gdrive\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "gdrive_path = Path('/gdrive')\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(str(gdrive_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7lH9_eAI3MbU"
   },
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iqRRjAmA_6Lk"
   },
   "outputs": [],
   "source": [
    "model_name = 'ce4718_hw04_lm.pt'\n",
    "if 'gdrive_path' in dir():\n",
    "  model_path = str(gdrive_path / 'My Drive' / model_name)\n",
    "else:\n",
    "  model_path = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4253815,
     "status": "ok",
     "timestamp": 1588459645556,
     "user": {
      "displayName": "sadra barikbein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCq0YqNHWfcZyly3I_AMTkvS8_tO8PZOQpGtkK=s64",
      "userId": "07513140462530453530"
     },
     "user_tz": -270
    },
    "id": "gt6THtBqGE0T",
    "outputId": "84c52db9-9cb1-425c-8037-7f3909511291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 12.976971626281738\n",
      "1 20.147899627685547\n",
      "2 13.183548927307129\n",
      "3 11.231220245361328\n",
      "4 9.24300765991211\n",
      "5 9.060257911682129\n",
      "6 5.5737833976745605\n",
      "7 4.911264896392822\n",
      "8 5.421207427978516\n",
      "9 3.6046457290649414\n",
      "10 5.028892993927002\n",
      "11 4.026007652282715\n",
      "12 2.8816051483154297\n",
      "13 3.1444742679595947\n",
      "14 3.2066948413848877\n",
      "15 3.7581605911254883\n",
      "16 3.7507827281951904\n",
      "17 2.663691520690918\n",
      "18 3.161072015762329\n",
      "19 1.8263498544692993\n",
      "20 2.6360294818878174\n",
      "21 2.0620479583740234\n",
      "22 2.090045690536499\n",
      "23 1.8589965105056763\n",
      "24 2.690744161605835\n",
      "25 1.8070528507232666\n",
      "26 2.2886829376220703\n",
      "27 2.0432486534118652\n",
      "28 1.5241689682006836\n",
      "29 1.9382303953170776\n",
      "30 2.2237048149108887\n",
      "31 2.5178678035736084\n",
      "32 1.7640352249145508\n",
      "33 1.6152228116989136\n",
      "34 1.8690558671951294\n",
      "35 1.5439138412475586\n",
      "36 1.309631586074829\n",
      "37 1.0194473266601562\n",
      "38 1.736807942390442\n",
      "39 1.9240063428878784\n",
      "40 1.3644100427627563\n",
      "41 1.7114938497543335\n",
      "42 1.4348763227462769\n",
      "43 1.2205826044082642\n",
      "44 1.2366944551467896\n",
      "45 1.0079509019851685\n",
      "46 1.0924385786056519\n",
      "47 1.1608518362045288\n",
      "48 1.1730467081069946\n",
      "49 1.1271413564682007\n",
      "50 1.0352096557617188\n",
      "51 1.230951189994812\n",
      "52 1.060771107673645\n",
      "53 1.337884783744812\n",
      "54 1.33164644241333\n",
      "55 0.9885631203651428\n",
      "56 1.1756529808044434\n",
      "57 1.1735366582870483\n",
      "58 1.2120295763015747\n",
      "59 1.2746901512145996\n",
      "60 0.9269230365753174\n",
      "61 0.9809961915016174\n",
      "62 1.1338388919830322\n",
      "63 1.060579538345337\n",
      "64 0.8760707974433899\n",
      "65 1.0614067316055298\n",
      "66 0.7768918871879578\n",
      "67 0.8918288350105286\n",
      "68 0.6275134682655334\n",
      "69 1.0563488006591797\n",
      "70 0.98488450050354\n",
      "71 0.9791233539581299\n",
      "72 0.7954891920089722\n",
      "73 0.8845797777175903\n",
      "74 0.7759691476821899\n",
      "75 1.0241169929504395\n",
      "76 0.7189197540283203\n",
      "77 0.7916702032089233\n",
      "78 0.5131152272224426\n",
      "79 0.8017325401306152\n",
      "80 0.7016087770462036\n",
      "81 0.8444517850875854\n",
      "82 0.9763853549957275\n",
      "83 0.9050127863883972\n",
      "84 0.9519914984703064\n",
      "85 0.7294919490814209\n",
      "86 1.7191133499145508\n",
      "87 0.6958268284797668\n",
      "88 0.7063570618629456\n",
      "89 1.5194907188415527\n",
      "90 0.7814981937408447\n",
      "91 0.923024594783783\n",
      "92 0.522785484790802\n",
      "93 0.689255952835083\n",
      "94 0.7786211967468262\n",
      "95 0.6618826985359192\n",
      "96 0.9056534171104431\n",
      "97 1.372429609298706\n",
      "98 0.5020797252655029\n",
      "99 0.7625159621238708\n",
      "100 0.5334120988845825\n",
      "101 0.5312575101852417\n",
      "102 0.5243204832077026\n",
      "103 0.5806097388267517\n",
      "104 0.5406650304794312\n",
      "105 0.6360099911689758\n",
      "106 0.586868941783905\n",
      "107 0.6418792605400085\n",
      "108 0.43573740124702454\n",
      "109 0.473911315202713\n",
      "110 0.5019768476486206\n",
      "111 0.6063934564590454\n",
      "112 0.46142661571502686\n",
      "113 0.47555261850357056\n",
      "114 0.4712083339691162\n",
      "115 0.5678501725196838\n",
      "116 0.6176798343658447\n",
      "117 0.8242247104644775\n",
      "118 0.5784724354743958\n",
      "119 0.5639188289642334\n",
      "120 0.47672581672668457\n",
      "121 0.4519420862197876\n",
      "122 0.3144932985305786\n",
      "123 0.5218204259872437\n",
      "124 0.4785315990447998\n",
      "125 0.4717255234718323\n",
      "126 0.5198110938072205\n",
      "127 0.578286349773407\n",
      "128 0.48958665132522583\n",
      "129 0.3586364984512329\n",
      "130 0.5217684507369995\n",
      "131 0.4552602767944336\n",
      "132 0.3046301603317261\n",
      "133 0.514193058013916\n",
      "134 0.4020001292228699\n",
      "135 0.3511661887168884\n",
      "136 0.48186445236206055\n",
      "137 0.9683862924575806\n",
      "138 0.4997482895851135\n",
      "139 0.38444778323173523\n",
      "140 0.411655992269516\n",
      "141 0.3631525933742523\n",
      "142 0.4405174255371094\n",
      "143 0.37576591968536377\n",
      "144 0.509003758430481\n",
      "145 0.3577859401702881\n",
      "146 0.3752078413963318\n",
      "147 0.31884291768074036\n",
      "148 0.49823394417762756\n",
      "149 0.3554309606552124\n",
      "150 0.36034131050109863\n",
      "151 0.914583146572113\n",
      "152 0.8581060171127319\n",
      "153 0.49375608563423157\n",
      "154 0.4195503890514374\n",
      "155 0.30411601066589355\n",
      "156 0.3223021626472473\n",
      "157 0.24861744046211243\n",
      "158 0.373933345079422\n",
      "159 0.4852718114852905\n",
      "160 0.4703846573829651\n",
      "161 0.418953001499176\n",
      "162 0.3202187418937683\n",
      "163 0.430824875831604\n",
      "164 0.3218068778514862\n",
      "165 0.35942763090133667\n",
      "166 0.33829265832901\n",
      "167 0.3665713369846344\n",
      "168 0.3355124592781067\n",
      "169 0.7650555968284607\n",
      "170 0.34598761796951294\n",
      "171 0.6449428200721741\n",
      "172 0.26650989055633545\n",
      "173 0.2998570203781128\n",
      "174 0.3937514126300812\n",
      "175 0.3371639847755432\n",
      "176 0.2097652405500412\n",
      "177 0.7976495027542114\n",
      "178 0.3092495799064636\n",
      "179 0.2847033739089966\n",
      "180 0.4027717709541321\n",
      "181 0.39061903953552246\n",
      "182 0.26227694749832153\n",
      "183 0.15612773597240448\n",
      "184 0.33578330278396606\n",
      "185 0.2940172851085663\n",
      "186 0.6793567538261414\n",
      "187 0.20098930597305298\n",
      "188 0.26855185627937317\n",
      "189 0.32334640622138977\n",
      "190 0.202241912484169\n",
      "191 0.3016582131385803\n",
      "192 0.23032788932323456\n",
      "193 0.5618144869804382\n",
      "194 0.8134877681732178\n",
      "195 0.3280754089355469\n",
      "196 0.24216927587985992\n",
      "197 0.32301127910614014\n",
      "198 0.14392653107643127\n",
      "199 0.2427511066198349\n",
      "200 0.2691017687320709\n",
      "201 0.2946351170539856\n",
      "202 0.29300928115844727\n",
      "203 0.21381007134914398\n",
      "204 0.13777701556682587\n",
      "205 0.22483569383621216\n",
      "206 0.18360324203968048\n",
      "207 0.1741664856672287\n",
      "208 0.16209828853607178\n",
      "209 0.24757422506809235\n",
      "210 0.1946941465139389\n",
      "211 0.14640243351459503\n",
      "212 0.1957508772611618\n",
      "213 0.20805442333221436\n",
      "214 0.133266419172287\n",
      "215 0.23034237325191498\n",
      "216 0.13820120692253113\n",
      "217 0.1665714532136917\n",
      "218 0.154772087931633\n",
      "219 0.5536202788352966\n",
      "220 0.13075682520866394\n",
      "221 0.14615410566329956\n",
      "222 0.15301798284053802\n",
      "223 0.12348255515098572\n",
      "224 0.15224409103393555\n",
      "225 0.11733092367649078\n",
      "226 0.10700532793998718\n",
      "227 0.17105209827423096\n",
      "228 0.13256964087486267\n",
      "229 0.16811123490333557\n",
      "230 0.15190823376178741\n",
      "231 0.08543296158313751\n",
      "232 0.12121731042861938\n",
      "233 0.46627017855644226\n",
      "234 0.18291564285755157\n",
      "235 0.08690015971660614\n",
      "236 0.076575368642807\n",
      "237 0.20130367577075958\n",
      "238 0.14272160828113556\n",
      "239 0.23893900215625763\n",
      "240 0.1610448658466339\n",
      "241 0.2052360326051712\n",
      "242 0.1338813453912735\n",
      "243 0.09887464344501495\n",
      "244 0.09143716096878052\n",
      "245 0.10314583778381348\n",
      "246 0.09954206645488739\n",
      "247 0.3649666905403137\n",
      "248 0.057283997535705566\n",
      "249 0.12969189882278442\n",
      "250 0.10261900722980499\n",
      "251 0.1459226906299591\n",
      "252 0.19714275002479553\n",
      "253 0.11982227861881256\n",
      "254 0.09482516348361969\n",
      "255 0.12933586537837982\n",
      "256 0.043361663818359375\n",
      "257 0.12188363075256348\n",
      "258 0.1656642109155655\n",
      "259 0.07265640795230865\n",
      "260 0.17952613532543182\n",
      "261 0.04555840790271759\n",
      "262 0.08591565489768982\n",
      "263 0.1613674759864807\n",
      "264 0.08712497353553772\n",
      "265 0.07929469645023346\n",
      "266 0.11587415635585785\n",
      "267 0.04696233570575714\n",
      "268 0.04405643045902252\n",
      "269 0.07517409324645996\n",
      "270 0.03900909423828125\n",
      "271 0.061157047748565674\n",
      "272 0.07626107335090637\n",
      "273 0.3532680869102478\n",
      "274 0.07274091243743896\n",
      "275 0.07235552370548248\n",
      "276 0.0329754501581192\n",
      "277 0.0399756133556366\n",
      "278 0.3402760624885559\n",
      "279 0.02670656144618988\n",
      "280 0.1507418304681778\n",
      "281 0.04985320568084717\n",
      "282 0.028399556875228882\n",
      "283 0.09341487288475037\n",
      "284 0.0317256897687912\n",
      "285 0.01823289692401886\n",
      "286 0.079744353890419\n",
      "287 0.08026427030563354\n",
      "288 0.08542665839195251\n",
      "289 0.04155072569847107\n",
      "290 0.02446533739566803\n",
      "291 0.08918304741382599\n",
      "292 0.047761738300323486\n",
      "293 0.0759044736623764\n",
      "294 0.32235291600227356\n",
      "295 0.05689132213592529\n",
      "296 0.021662801504135132\n",
      "297 0.067079097032547\n",
      "298 0.05791302025318146\n",
      "299 0.004369303584098816\n",
      "300 -0.020547285676002502\n",
      "301 0.043821513652801514\n",
      "302 -0.00951702892780304\n",
      "303 -0.04063820838928223\n",
      "304 0.03937119245529175\n",
      "305 0.04608540236949921\n",
      "306 -0.029994472861289978\n",
      "307 -0.017473652958869934\n",
      "308 -0.024585038423538208\n",
      "309 0.002821028232574463\n",
      "310 0.006081774830818176\n",
      "311 3.929436206817627e-05\n",
      "312 -0.009174197912216187\n",
      "313 0.04124248027801514\n",
      "314 0.02663823962211609\n",
      "315 0.01687219738960266\n",
      "316 0.24095499515533447\n",
      "317 -0.04813796281814575\n",
      "318 -0.0055616796016693115\n",
      "319 0.2027534395456314\n",
      "320 -0.032724618911743164\n",
      "321 -0.02355511486530304\n",
      "322 -0.04826773703098297\n",
      "323 -0.03871743381023407\n",
      "324 -0.039479419589042664\n",
      "325 -0.06520707905292511\n",
      "326 2.022087574005127e-05\n",
      "327 -0.021952465176582336\n",
      "328 -0.026948705315589905\n",
      "329 -0.029613137245178223\n",
      "330 0.004867345094680786\n",
      "331 -0.052428483963012695\n",
      "332 -0.01913677155971527\n",
      "333 -0.02414083480834961\n",
      "334 -0.034755438566207886\n",
      "335 -0.026888534426689148\n",
      "336 -0.03465753793716431\n",
      "337 -0.05667950212955475\n",
      "338 -0.016938447952270508\n",
      "339 -0.08023864030838013\n",
      "340 -0.10022635757923126\n",
      "341 -0.06524041295051575\n",
      "342 -0.06475423276424408\n",
      "343 -0.09670248627662659\n",
      "344 -0.03340707719326019\n",
      "345 -0.048349156975746155\n",
      "346 -0.055296897888183594\n",
      "347 0.10959349572658539\n",
      "348 -0.06913216412067413\n",
      "349 -0.10392370820045471\n",
      "350 -0.08042772114276886\n",
      "351 -0.04781694710254669\n",
      "352 -0.09505069255828857\n",
      "353 -0.04649195075035095\n",
      "354 -0.05871093273162842\n",
      "355 -0.07859180867671967\n",
      "356 -0.08683256804943085\n",
      "357 0.16177132725715637\n",
      "358 -0.08462154865264893\n",
      "359 -0.08317255973815918\n",
      "360 -0.06097795069217682\n",
      "361 -0.13206423819065094\n",
      "362 -0.07157972455024719\n",
      "363 -0.10162191092967987\n",
      "364 -0.07904274761676788\n",
      "365 -0.08392031490802765\n",
      "366 -0.05204898118972778\n",
      "367 -0.06478554010391235\n",
      "368 -0.09963136911392212\n",
      "369 -0.09269702434539795\n",
      "370 -0.10562452673912048\n",
      "371 -0.08479675650596619\n",
      "372 -0.10524694621562958\n",
      "373 -0.1051146388053894\n",
      "374 -0.12168048322200775\n",
      "375 -0.06789271533489227\n",
      "376 -0.07900826632976532\n",
      "377 -0.09618696570396423\n",
      "378 -0.0990348607301712\n",
      "379 -0.14587263762950897\n",
      "380 -0.11686043441295624\n",
      "381 -0.13494320213794708\n",
      "382 -0.11607974767684937\n",
      "383 -0.14561350643634796\n",
      "384 -0.08695513010025024\n",
      "385 -0.12030519545078278\n",
      "386 -0.09444560110569\n",
      "387 -0.10161162912845612\n",
      "388 -0.12194450199604034\n",
      "389 -0.11915218830108643\n",
      "390 -0.16380968689918518\n",
      "391 -0.13059112429618835\n",
      "392 -0.11403931677341461\n",
      "393 -0.10113967955112457\n",
      "394 -0.15471458435058594\n",
      "395 -0.0983053594827652\n",
      "396 -0.07049790024757385\n",
      "397 -0.11924201250076294\n",
      "398 -0.20297196507453918\n",
      "399 -0.12857425212860107\n",
      "400 -0.1373603343963623\n",
      "401 -0.16339334845542908\n",
      "402 0.04119068384170532\n",
      "403 -0.11165709793567657\n",
      "404 -0.1518026441335678\n",
      "405 -0.1082834005355835\n",
      "406 -0.10900610685348511\n",
      "407 -0.0867096483707428\n",
      "408 -0.10677273571491241\n",
      "409 -0.1486033797264099\n",
      "410 -0.14425544440746307\n",
      "411 -0.13171672821044922\n",
      "412 0.0417407751083374\n",
      "413 -0.12528520822525024\n",
      "414 -0.11828581988811493\n",
      "415 -0.14492294192314148\n",
      "416 -0.1515686810016632\n",
      "417 -0.1385239213705063\n",
      "418 -0.15887342393398285\n",
      "419 -0.10880060493946075\n",
      "420 -0.18683049082756042\n",
      "421 -0.14739032089710236\n",
      "422 -0.1443110853433609\n",
      "423 -0.13534121215343475\n",
      "424 -0.16153982281684875\n",
      "425 -0.16886159777641296\n",
      "426 -0.13672102987766266\n",
      "427 -0.16606168448925018\n",
      "428 -0.020447731018066406\n",
      "429 -0.12644965946674347\n",
      "430 -0.18138554692268372\n",
      "431 -0.19342243671417236\n",
      "432 -0.1742800623178482\n",
      "433 -0.1748725324869156\n",
      "434 -0.16350805759429932\n",
      "435 -0.1771981567144394\n",
      "436 -0.1737891435623169\n",
      "437 -0.16115924715995789\n",
      "438 -0.13617748022079468\n",
      "439 -0.1681627333164215\n",
      "440 -0.1707288771867752\n",
      "441 -0.17898106575012207\n",
      "442 -0.19409392774105072\n",
      "443 -0.19572706520557404\n",
      "444 -0.19138339161872864\n",
      "445 -0.140512615442276\n",
      "446 -0.22313085198402405\n",
      "447 -0.17499753832817078\n",
      "448 -0.13268037140369415\n",
      "449 -0.17230382561683655\n",
      "450 -0.17345477640628815\n",
      "451 -0.14417365193367004\n",
      "452 -0.1805943101644516\n",
      "453 -0.1797313392162323\n",
      "454 -0.1734057366847992\n",
      "455 -0.1952301263809204\n",
      "456 -0.1801505982875824\n",
      "457 -0.16765186190605164\n",
      "458 -0.06407797336578369\n",
      "459 -0.14755333960056305\n",
      "460 -0.17963644862174988\n",
      "461 -0.18929217755794525\n",
      "462 -0.20734485983848572\n",
      "463 -0.1913638859987259\n",
      "464 -0.18265379965305328\n",
      "465 -0.18734757602214813\n",
      "466 -0.1916314959526062\n",
      "467 -0.1924089938402176\n",
      "468 -0.21710260212421417\n",
      "469 -0.19312627613544464\n",
      "470 -0.19123539328575134\n",
      "471 -0.2347922921180725\n",
      "472 -0.20880503952503204\n",
      "473 -0.2071860283613205\n",
      "474 -0.22137966752052307\n",
      "475 -0.19568559527397156\n",
      "476 -0.1926322877407074\n",
      "477 -0.17931069433689117\n",
      "478 -0.2308173030614853\n",
      "479 -0.21309205889701843\n",
      "480 -0.17081481218338013\n",
      "481 -0.2140117585659027\n",
      "482 -0.2234296202659607\n",
      "483 -0.18671339750289917\n",
      "484 -0.2154092639684677\n",
      "485 -0.22831156849861145\n",
      "486 -0.20963124930858612\n",
      "487 -0.21507857739925385\n",
      "488 -0.21401111781597137\n",
      "489 -0.20346693694591522\n",
      "490 -0.21885617077350616\n",
      "491 -0.23258917033672333\n",
      "492 -0.23463886976242065\n",
      "493 -0.23109248280525208\n",
      "494 -0.1940782368183136\n",
      "495 -0.08237254619598389\n",
      "496 -0.2329907864332199\n",
      "497 -0.22643375396728516\n",
      "498 -0.24348238110542297\n",
      "499 -0.2348615527153015\n",
      "500 -0.23353558778762817\n",
      "501 -0.1984661966562271\n",
      "502 -0.24839523434638977\n",
      "503 -0.2422688603401184\n",
      "504 -0.24600407481193542\n",
      "505 -0.2096841037273407\n",
      "506 -0.20592375099658966\n",
      "507 -0.23310938477516174\n",
      "508 -0.2543601989746094\n",
      "509 -0.2527177631855011\n",
      "510 -0.2128341645002365\n",
      "511 -0.22193577885627747\n",
      "512 -0.24469739198684692\n",
      "513 -0.24550387263298035\n",
      "514 -0.2560420036315918\n",
      "515 -0.25607675313949585\n",
      "516 -0.2342231273651123\n",
      "517 -0.2356630563735962\n",
      "518 -0.24521404504776\n",
      "519 -0.2697984576225281\n",
      "520 -0.24634814262390137\n",
      "521 -0.2304585576057434\n",
      "522 -0.23312661051750183\n",
      "523 -0.2508552074432373\n",
      "524 -0.2451583445072174\n",
      "525 -0.2606842815876007\n",
      "526 -0.25763779878616333\n",
      "527 -0.23510012030601501\n",
      "528 -0.09513476490974426\n",
      "529 -0.21051758527755737\n",
      "530 -0.2270461767911911\n",
      "531 -0.2693149447441101\n",
      "532 -0.24825306236743927\n",
      "533 -0.2694683074951172\n",
      "534 -0.23305079340934753\n",
      "535 -0.27360260486602783\n",
      "536 -0.2659105360507965\n",
      "537 -0.24016283452510834\n",
      "538 -0.27291810512542725\n",
      "539 -0.25026440620422363\n",
      "540 -0.28969067335128784\n",
      "541 -0.2834509313106537\n",
      "542 -0.28056737780570984\n",
      "543 -0.2588714063167572\n",
      "544 -0.259158730506897\n",
      "545 -0.296032190322876\n",
      "546 -0.28296977281570435\n",
      "547 -0.2575588822364807\n",
      "548 -0.26791611313819885\n",
      "549 -0.27446746826171875\n",
      "550 -0.2894073724746704\n",
      "551 -0.2774391174316406\n",
      "552 -0.247431680560112\n",
      "553 -0.27112436294555664\n",
      "554 -0.2515996992588043\n",
      "555 -0.3029334247112274\n",
      "556 -0.2563226521015167\n",
      "557 -0.26721301674842834\n",
      "558 -0.283381849527359\n",
      "559 -0.15491384267807007\n",
      "560 -0.29704317450523376\n",
      "561 -0.2662164568901062\n",
      "562 -0.2932840585708618\n",
      "563 -0.2961210012435913\n",
      "564 -0.2691884934902191\n",
      "565 -0.2976225018501282\n",
      "566 -0.2762985825538635\n",
      "567 -0.28303319215774536\n",
      "568 -0.28259891271591187\n",
      "569 -0.31944820284843445\n",
      "570 -0.3063942492008209\n",
      "571 -0.29213038086891174\n",
      "572 -0.2710254192352295\n",
      "573 -0.30268770456314087\n",
      "574 -0.2760067582130432\n",
      "575 -0.33847397565841675\n",
      "576 -0.3146573007106781\n",
      "577 -0.3126372992992401\n",
      "578 -0.33034971356391907\n",
      "579 -0.30032914876937866\n",
      "580 -0.30810976028442383\n",
      "581 -0.2914157807826996\n",
      "582 -0.28265678882598877\n",
      "583 -0.30576327443122864\n",
      "584 -0.2954699993133545\n",
      "585 -0.31551501154899597\n",
      "586 -0.3266495168209076\n",
      "587 -0.30703336000442505\n",
      "588 -0.3048819303512573\n",
      "589 -0.30921468138694763\n",
      "590 -0.28480327129364014\n",
      "591 -0.3118123710155487\n",
      "592 -0.30291038751602173\n",
      "593 -0.30649131536483765\n",
      "594 -0.3146960139274597\n",
      "595 -0.3095894455909729\n",
      "596 -0.317868173122406\n",
      "597 -0.2876347005367279\n",
      "598 -0.3510757088661194\n",
      "599 -0.31447869539260864\n",
      "600 -0.23229816555976868\n",
      "601 -0.32052069902420044\n",
      "602 -0.3100985586643219\n",
      "603 -0.31935369968414307\n",
      "604 -0.3266472816467285\n",
      "605 -0.33029717206954956\n",
      "606 -0.3315669298171997\n",
      "607 -0.3091539144515991\n",
      "608 -0.31926119327545166\n",
      "609 -0.3584345281124115\n",
      "610 -0.3355482816696167\n",
      "611 -0.3530619144439697\n",
      "612 -0.34408798813819885\n",
      "613 -0.3155011236667633\n",
      "614 -0.3215475380420685\n",
      "615 -0.3406462073326111\n",
      "616 -0.3155233860015869\n",
      "617 -0.3144455850124359\n",
      "618 -0.36185479164123535\n",
      "619 -0.3414744734764099\n",
      "620 -0.33536678552627563\n",
      "621 -0.329826295375824\n",
      "622 -0.31875500082969666\n",
      "623 -0.3321443796157837\n",
      "624 -0.3645165264606476\n",
      "625 -0.32868677377700806\n",
      "626 -0.3480527102947235\n",
      "627 -0.35787832736968994\n",
      "628 -0.3268803358078003\n",
      "629 -0.36386874318122864\n",
      "630 -0.3489522337913513\n",
      "631 -0.36494410037994385\n",
      "632 -0.3710634708404541\n",
      "633 -0.35262060165405273\n",
      "634 -0.35646599531173706\n",
      "635 -0.3600612282752991\n",
      "636 -0.35101234912872314\n",
      "637 -0.3623519837856293\n",
      "638 -0.3707013428211212\n",
      "639 -0.35076430439949036\n",
      "640 -0.25054100155830383\n",
      "641 -0.36606067419052124\n",
      "642 -0.35303324460983276\n",
      "643 -0.2892119288444519\n",
      "644 -0.3310094475746155\n",
      "645 -0.35464656352996826\n",
      "646 -0.3590995669364929\n",
      "647 -0.3418181836605072\n",
      "648 -0.36483144760131836\n",
      "649 -0.35574257373809814\n",
      "650 -0.3696068227291107\n",
      "651 -0.3553827106952667\n",
      "652 -0.36199381947517395\n",
      "653 -0.37917521595954895\n",
      "654 -0.3584330379962921\n",
      "655 -0.2603876292705536\n",
      "656 -0.38435274362564087\n",
      "657 -0.37641623616218567\n",
      "658 -0.3862904906272888\n",
      "659 -0.35387223958969116\n",
      "660 -0.3741524815559387\n",
      "661 -0.37198078632354736\n",
      "662 -0.36587101221084595\n",
      "663 -0.3606681227684021\n",
      "664 -0.3881933093070984\n",
      "665 -0.3632849454879761\n",
      "666 -0.3599977195262909\n",
      "667 -0.2793503403663635\n",
      "668 -0.37808823585510254\n",
      "669 -0.36870092153549194\n",
      "670 -0.3754468262195587\n",
      "671 -0.36789047718048096\n",
      "672 -0.3733050227165222\n",
      "673 -0.3663715720176697\n",
      "674 -0.3752427101135254\n",
      "675 -0.37541094422340393\n",
      "676 -0.38762176036834717\n",
      "677 -0.3787309229373932\n",
      "678 -0.3838094472885132\n",
      "679 -0.3937925100326538\n",
      "680 -0.39442458748817444\n",
      "681 -0.3975284993648529\n",
      "682 -0.41447246074676514\n",
      "683 -0.4081699252128601\n",
      "684 -0.39913666248321533\n",
      "685 -0.315945565700531\n",
      "686 -0.40136924386024475\n",
      "687 -0.3908347487449646\n",
      "688 -0.42086151242256165\n",
      "689 -0.3964627683162689\n",
      "690 -0.4197351932525635\n",
      "691 -0.3175601065158844\n",
      "692 -0.38796699047088623\n",
      "693 -0.39430296421051025\n",
      "694 -0.40404456853866577\n",
      "695 -0.38380134105682373\n",
      "696 -0.4154520332813263\n",
      "697 -0.3972746729850769\n",
      "698 -0.40253540873527527\n",
      "699 -0.4246116876602173\n",
      "700 -0.41629496216773987\n",
      "701 -0.41854745149612427\n",
      "702 -0.4055561125278473\n",
      "703 -0.4084046185016632\n",
      "704 -0.40138328075408936\n",
      "705 -0.40338531136512756\n",
      "706 -0.41070324182510376\n",
      "707 -0.4035367965698242\n",
      "708 -0.41455262899398804\n",
      "709 -0.45583054423332214\n",
      "710 -0.40186816453933716\n",
      "711 -0.42600899934768677\n",
      "712 -0.42628395557403564\n",
      "713 -0.42260289192199707\n",
      "714 -0.4255599081516266\n",
      "715 -0.45253995060920715\n",
      "716 -0.41573387384414673\n",
      "717 -0.4021798372268677\n",
      "718 -0.41562047600746155\n",
      "719 -0.41947364807128906\n",
      "720 -0.4483819603919983\n",
      "721 -0.44090306758880615\n",
      "722 -0.42743992805480957\n",
      "723 -0.4207867383956909\n",
      "724 -0.44462472200393677\n",
      "725 -0.4211764335632324\n",
      "726 -0.43884801864624023\n",
      "727 -0.42160362005233765\n",
      "728 -0.40986013412475586\n",
      "729 -0.4243238568305969\n",
      "730 -0.42166900634765625\n",
      "731 -0.4981284439563751\n",
      "Epoch 1: Train Loss: 0.2540316451035562\n",
      "Epoch 2: Train Loss: -0.7359044190536264\n",
      "Epoch 3: Train Loss: -1.3422023617328807\n",
      "Epoch 4: Train Loss: -1.9709224946167554\n",
      "Epoch 5: Train Loss: -2.6232148355512477\n",
      "Epoch 6: Train Loss: -3.320524861619633\n",
      "Epoch 7: Train Loss: -4.091491957858877\n",
      "Epoch 8: Train Loss: -4.946054670618269\n",
      "Epoch 9: Train Loss: -5.816090775464605\n",
      "Epoch 10: Train Loss: -6.679473310426705\n",
      "Epoch 11: Train Loss: -7.413597697458085\n",
      "Epoch 12: Train Loss: -8.009208161471461\n",
      "Epoch 13: Train Loss: -8.555452007047084\n",
      "Epoch 14: Train Loss: -9.085219789745036\n",
      "Epoch 15: Train Loss: -9.608458235880256\n",
      "Epoch 16: Train Loss: -10.127106224394957\n",
      "Epoch 17: Train Loss: -10.647397781639407\n",
      "Epoch 18: Train Loss: -11.166133937861714\n",
      "Epoch 19: Train Loss: -11.679079449176788\n",
      "Epoch 20: Train Loss: -12.190095782763903\n",
      "Epoch 21: Train Loss: -12.703582410017649\n",
      "Epoch 22: Train Loss: -13.216671944956792\n",
      "Epoch 23: Train Loss: -13.736107523499783\n",
      "Epoch 24: Train Loss: -14.25050186982026\n",
      "Epoch 25: Train Loss: -14.771736956354397\n",
      "Epoch 26: Train Loss: -15.290221028243373\n",
      "Epoch 27: Train Loss: -15.80260107381557\n",
      "Epoch 28: Train Loss: -16.32775477304833\n",
      "Epoch 29: Train Loss: -16.843117548795355\n",
      "Epoch 30: Train Loss: -17.366716691196046\n",
      "Epoch 31: Train Loss: -17.888232550763956\n",
      "Epoch 32: Train Loss: -18.394138085352232\n",
      "Epoch 33: Train Loss: -18.912777193300016\n",
      "Epoch 34: Train Loss: -19.421098918166308\n",
      "Epoch 35: Train Loss: -19.949054712862583\n",
      "Epoch 36: Train Loss: -20.475042350840923\n",
      "Epoch 37: Train Loss: -20.984697241809485\n",
      "Epoch 38: Train Loss: -21.499845350605167\n",
      "Epoch 39: Train Loss: -22.00692961936773\n",
      "Epoch 40: Train Loss: -22.5205417585767\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss = -22.5205417585767 train_ppl = 12.812679431463204\n"
     ]
    }
   ],
   "source": [
    "loss, ppl, model = train(model_path)\n",
    "print(\"train_loss =\", loss, \"train_ppl =\", ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ByNWh4kmLUMz"
   },
   "outputs": [],
   "source": [
    "# Clear the GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4yjSXSxZSVBo"
   },
   "source": [
    "Measure the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5133,
     "status": "ok",
     "timestamp": 1588459786780,
     "user": {
      "displayName": "sadra barikbein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCq0YqNHWfcZyly3I_AMTkvS8_tO8PZOQpGtkK=s64",
      "userId": "07513140462530453530"
     },
     "user_tz": -270
    },
    "id": "RTbdO6OVSYcg",
    "outputId": "8f24a1cc-b1a3-4d15-f4e3-263c73e6d6ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation : Validation loss: -23.36527592884329\n",
      "valid_loss = -23.36527592884329 valid_ppl = 1004.1850156634785\n"
     ]
    }
   ],
   "source": [
    "valid_ds = LMDataset('wikitext-2/valid.txt', word2id=word2id)\n",
    "val_loss, val_ppl = evaluate(model, valid_ds)\n",
    "print(\"valid_loss =\", val_loss, \"valid_ppl =\", val_ppl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmtZqJ4lQqfy"
   },
   "source": [
    "### 1.4 Text Generation (Optional: 1 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJ-fGqs_XvlO"
   },
   "source": [
    "Now, as the model has been trained on the dataset, it would be a good time to have some fun with it. In this section, we are going to use our language model to generate text. Specifically, given a small span of text, we want our model to complete the rest of it. To implement this, we simply input the initial text to the model, and then we predict the next words step-by-step. Predicting the next word can be achieved by taking the word that has the highest probability in the model's outputs (i.e., using `argmax`). However, it has been shown that naively taking the most probable word is not the best way to do it. Instead, we should sample the next word from the distribution the model puts on the vocabulary for each step (i.e., sampling from the model output).\n",
    "\n",
    "Implement this mechanism in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Muyr8TkXuga"
   },
   "outputs": [],
   "source": [
    "def generate(input_words, model,  word2id, id2word, max_length, temperature):\n",
    "  \"\"\"\n",
    "  Generate text given the input words\n",
    "\n",
    "  Args:\n",
    "    input_words (str): the input words. It needs tokenization\n",
    "    model (ExtendedLM): the model\n",
    "    word2id (Dict[str, int]), id2word (List[str]): the vocab\n",
    "    max_length (int): the maximum number of words to generate\n",
    "    temperature (float): the softmax temperature. A value between 0 and 1.\n",
    "          see https://en.wikipedia.org/wiki/Softmax_function for more information.\n",
    "\n",
    "  Returns:\n",
    "    List[str]: generated text\n",
    "  \"\"\"\n",
    "  tokens=word_tokenize(input_words)\n",
    "  ids=torch.tensor(list(map(lambda t:word2id[t] if t in word2id else 0,tokens)),device=DEVICE).reshape(1,-1)\n",
    "  count=0\n",
    "  model.eval()\n",
    "  hiddens=list(map(lambda hc:(hc[0].to(DEVICE),hc[1].to(DEVICE)),model.init_zero_state(1)))\n",
    "  result=[]\n",
    "  while count<=max_length:\n",
    "    preds,hiddens,_=model(ids,hiddens)\n",
    "    beta=1./temperature\n",
    "    preds=F.softmax(beta*preds.squeeze(0))\n",
    "    word=[id2word[w.item()] for w in Categorical(preds).sample()][-1]\n",
    "    result+=[word]\n",
    "    ids[0]=torch.cat([ids[0,1:],torch.tensor([word2id[word]],device=DEVICE)])\n",
    "    count+=1\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2am-drncrP7v"
   },
   "source": [
    "Check your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1119,
     "status": "ok",
     "timestamp": 1588459807854,
     "user": {
      "displayName": "sadra barikbein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCq0YqNHWfcZyly3I_AMTkvS8_tO8PZOQpGtkK=s64",
      "userId": "07513140462530453530"
     },
     "user_tz": -270
    },
    "id": "jsQtHocnB1kf",
    "outputId": "196422e2-7b79-4ec0-d253-7e8d5c1fad2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', 'the', '.', ')', '.', 'the', 'it', 'to', 'on', ',', ',']"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(\"A complete edition including the three downloadable content packs was released a year \", model, word2id, id2word, 10, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1104,
     "status": "ok",
     "timestamp": 1588459815001,
     "user": {
      "displayName": "sadra barikbein",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCq0YqNHWfcZyly3I_AMTkvS8_tO8PZOQpGtkK=s64",
      "userId": "07513140462530453530"
     },
     "user_tz": -270
    },
    "id": "wT-v6nOprSJc",
    "outputId": "1a1d9553-9972-4c10-aeb0-114dff60a270"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at', ',', '.', 'that', ',', '<unk>', ',', ',', '<unk>', 'of', 'and']"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('Environment is at risk!',\n",
    "         model, word2id, id2word, 10, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MG0_Hr56pJj8"
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t3r5rMBbqNpz"
   },
   "source": [
    "- Check and review your answers. Make sure all cells' output are what you have planned.\n",
    "- Select File > Save.\n",
    "- To download the notebook, select File > Download .ipynb.\n",
    "- Create an archive of all notebooks (P1.ipynb, P2.ipynb, and P3.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccYOoVeEMg2V"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JjE3oxWjjcf1"
   },
   "source": [
    "1.   Gal, Yarin, and Zoubin Ghahramani. “A Theoretically Grounded Application of Dropout in Recurrent Neural Networks.” ArXiv:1512.05287 [Stat], October 5, 2016. http://arxiv.org/abs/1512.05287.\n",
    "2.  Inan, Hakan, Khashayar Khosravi, and Richard Socher. “TYING WORD VECTORS AND WORD CLASSIFIERS: A LOSS FRAMEWORK FOR LANGUAGE MODELING.” In ICLR 2017, 13, 2017.\n",
    "3. Merity, Stephen, Bryan McCann, and Richard Socher. “Revisiting Activation Regularization for Language RNNs.” ArXiv:1708.01009 [Cs], August 3, 2017. http://arxiv.org/abs/1708.01009.\n",
    "4. Merity, Stephen, Nitish Shirish Keskar, and Richard Socher. “Regularizing and Optimizing LSTM Language Models.” ArXiv:1708.02182 [Cs], August 7, 2017. http://arxiv.org/abs/1708.02182.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW4_Part1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
