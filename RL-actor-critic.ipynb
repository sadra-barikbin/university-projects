{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "96Sc-xN7OXqV"
   },
   "source": [
    "# CE-40719: Deep Learning\n",
    "## HW6 - Deep Reinforcement Learning\n",
    "(20 points)\n",
    "\n",
    "#### Name: Sadroddin Barikbin\n",
    "#### Student No.: 98208824"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-n6ALmbHO3Sb"
   },
   "source": [
    "In this assignment we are going to train a simple Actor-Critic model to solve classical control problems. We are going to use a batch version of the standard [gym](https://gym.openai.com/) library that is given to you in `multi_env.py`. The only difference between these two versions is that in `multi_env.py` instead of a single environment we have a batch of environments, therefore the observations are in shape `(batch_size * observation_size)`. We will focus on `CartPole-v1` problem but you can apply this to other problems as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jJSSI4_tlGAi"
   },
   "source": [
    "## Algorithm\n",
    "\n",
    "The vanilla actor-critic algorithm is as follows:\n",
    "\n",
    "1.   Sample a batch $\\{(s_i, a_i, r_i, s_{i + 1})\\}_i$ under policy $\\pi_\\theta$.\n",
    "2.   Fit $V_\\phi^{\\pi_\\theta}(s_i)$ to $r_i + \\gamma V_\\phi^{\\pi_\\theta}(s_{i+1})$ by minimizing squared error $\\|r_i + \\gamma V_\\phi^{\\pi_\\theta}(s_{i+1})- V_\\phi^{\\pi_\\theta}(s_i)\\|^2$.\n",
    "3. $\\max_{\\theta}~ \\sum_{i} \\log \\pi_\\theta(a_i|s_i) \\left[ r_i + \\gamma V_\\phi^{\\pi_\\theta}(s_{i+1})- V^{\\pi_\\theta}_\\phi(s_i) \\right]$\n",
    "\n",
    "We need two parametrized models, one for value function $V^{\\pi_\\theta}_\\phi$ and one for stochastic policy $\\pi_\\theta$. Since both $\\pi_\\theta$ and $V^{\\pi_\\theta}_\\phi$ are functions of state $s$, instead of modeling each with a seperate neural network, we can model both with a single network with shared parameters. In other words we train a single network that outputs both $\\pi_\\theta(a|s)$ and $V^{\\pi_\\theta}_\\phi(s)$. To train this network we combine step 2 and 3 in the main algoritm and optimize the following objective:\n",
    "$$\\min_{\\theta, \\phi}~ -\\sum_{i} \\log \\pi_\\theta(a_i|s_i) \\left[ r_i + \\gamma V_\\phi^{\\pi_\\theta}(s_{i+1})- V^{\\pi_\\theta}_\\phi(s_i) \\right] + \\|r_i + \\gamma V_\\phi^{\\pi_\\theta}(s_{i+1})- V_\\phi^{\\pi_\\theta}(s_i)\\|^2$$\n",
    "\n",
    "Note that the gradient must be backpropagated only through $\\log \\pi_\\theta(a_i|s_i)$ and $V_\\phi^{\\pi_\\theta}(s_i)$ in the squared error. A negative entropy term $-\\mathcal{H} (\\pi_\\theta(a_i|s_i))$ can also be added to above objective to encourage exploration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dTSnFyiOyz7o"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_IYE3nBRhLWX"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "\n",
    "from multi_env import SubprocVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hj9M77VaHsP7"
   },
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "num_envs = 16\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pHy1yIj2CC31"
   },
   "source": [
    "## 1. Model (8 Points)\n",
    "\n",
    "To define a stochastic policy we use [`torch.distributions`](https://pytorch.org/docs/stable/distributions.html) module. Networks shared parameters are defined in a simple MLP. Network has two heads, one for $V$ that takes in MLPs output and outputs a scalar, and one for $\\pi$ that takes in the MLPs output and outputs a categorical distribution for each action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6aeF3ISTXK_"
   },
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_size, hidden_size, num_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        #################################################################################\n",
    "        #                          COMPLETE THE FOLLOWING SECTION                       #\n",
    "        #################################################################################\n",
    "        # state_size: size of the input state\n",
    "        # hidden_size: a list containing size of each mlp hidden layer in order\n",
    "        # num_action: number of actions\n",
    "        # do not use batch norm for any layer in this network\n",
    "        #################################################################################\n",
    "        hid_inp=state_size\n",
    "        self.fcs=[]\n",
    "        for hidden in hidden_size:\n",
    "          self.fcs+=[nn.Linear(hid_inp,hidden)]\n",
    "          hid_inp=hidden\n",
    "        self.fcs=nn.ModuleList(self.fcs)\n",
    "        self.fcv=nn.Linear(hid_inp,1)\n",
    "        self.fca=nn.Linear(hid_inp,num_actions)\n",
    "        #################################################################################\n",
    "        #                                   THE END                                     #\n",
    "        #################################################################################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        #################################################################################\n",
    "        #                          COMPLETE THE FOLLOWING SECTION                       #\n",
    "        #################################################################################\n",
    "        for linear in self.fcs:\n",
    "          x=F.relu(linear(x))\n",
    "        policy=F.softmax(self.fca(x),dim=-1)\n",
    "        value=self.fcv(x)\n",
    "        #################################################################################\n",
    "        #                                   THE END                                     #\n",
    "        #################################################################################\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K0U1wUC0QhaT"
   },
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    env = gym.make(env_name)\n",
    "    total_reward = 0\n",
    "    #################################################################################\n",
    "    #                          COMPLETE THE FOLLOWING SECTION                       #\n",
    "    #################################################################################\n",
    "    # run given model for a single episode and compute total reward.\n",
    "    #################################################################################\n",
    "    done=False\n",
    "    obs=[torch.FloatTensor(env.reset())] * num_state_obs\n",
    "    while not done:\n",
    "      state=torch.cat(obs,dim=-1).to(device)\n",
    "      actions,v=model(state)\n",
    "      action=torch.distributions.Categorical(actions).sample().item()\n",
    "      ob,reward,done,_=env.step(action)\n",
    "      obs=obs[1:]+[torch.FloatTensor(ob)]\n",
    "      total_reward+=reward      \n",
    "    #################################################################################\n",
    "    #                                   THE END                                     #\n",
    "    #################################################################################\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GP3XquD5zL-8"
   },
   "source": [
    "## 2. Objective and Training (12 Points)\n",
    "\n",
    "A single observation is not always enough to understand state of an environment, hence we take previous `num_state_obs` observations at time t as state of the environment at time t. Initialize and train the model using Adam optimizer. You should be able to get to 500 in less than 20000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "id": "IMhtTYWjz4X8"
   },
   "outputs": [],
   "source": [
    "num_state_obs =  10\n",
    "gamma = 0.99\n",
    "#################################################################################\n",
    "#                          COMPLETE THE FOLLOWING SECTION                       #\n",
    "#################################################################################\n",
    "# experiment with different parameters and models to get the best result\n",
    "#################################################################################\n",
    "num_iterations = 20000\n",
    "\n",
    "obs_size = 4\n",
    "state_size = num_state_obs*obs_size\n",
    "num_actions = 2\n",
    "\n",
    "model = ActorCritic(state_size,[30,20],num_actions)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "#################################################################################\n",
    "#                                   THE END                                     #\n",
    "#################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 223266,
     "status": "ok",
     "timestamp": 1592026506557,
     "user": {
      "displayName": "sadra barikbin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiCq0YqNHWfcZyly3I_AMTkvS8_tO8PZOQpGtkK=s64",
      "userId": "07513140462530453530"
     },
     "user_tz": -270
    },
    "id": "vArHT-zznKWL",
    "outputId": "9dc803b3-4040-4cae-fb77-66b4faed26f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1000: average reward = 27.800000\n",
      "iteration  2000: average reward = 14.300000\n",
      "iteration  3000: average reward = 18.900000\n",
      "iteration  4000: average reward = 191.700000\n",
      "iteration  5000: average reward = 331.600000\n",
      "iteration  6000: average reward = 492.200000\n",
      "iteration  7000: average reward = 500.000000\n",
      "iteration  8000: average reward = 32.600000\n",
      "iteration  9000: average reward = 10.500000\n",
      "iteration 10000: average reward = 500.000000\n",
      "iteration 11000: average reward = 500.000000\n",
      "iteration 12000: average reward = 500.000000\n",
      "iteration 13000: average reward = 500.000000\n",
      "iteration 14000: average reward = 500.000000\n",
      "iteration 15000: average reward = 314.400000\n",
      "iteration 16000: average reward = 221.000000\n",
      "iteration 17000: average reward = 136.800000\n",
      "iteration 18000: average reward = 500.000000\n",
      "iteration 19000: average reward = 500.000000\n",
      "iteration 20000: average reward = 124.800000\n"
     ]
    }
   ],
   "source": [
    "obs = [torch.FloatTensor(envs.reset())] * num_state_obs\n",
    "for t in range(num_iterations):\n",
    "    model.train()\n",
    "    #################################################################################\n",
    "    #                          COMPLETE THE FOLLOWING SECTION                       #\n",
    "    #################################################################################\n",
    "    # implement the algorithm\n",
    "    #################################################################################\n",
    "    state=torch.cat(obs,dim=-1).to(device)\n",
    "    actions,v=model(state)\n",
    "    action=torch.distributions.Categorical(actions).sample()\n",
    "    ob,reward,done,_=envs.step(action.cpu().numpy())\n",
    "    obs=obs[1:]+[torch.FloatTensor(ob)]\n",
    "\n",
    "    state_=torch.cat(obs,dim=-1).to(device)\n",
    "    _,v_=model(state_)\n",
    "    v_=v_.detach()\n",
    "    actions,v=model(state)\n",
    "    reward=torch.tensor(reward,device=device,dtype=torch.float32).unsqueeze(dim=-1)\n",
    "    target=reward+gamma*v_*(~ torch.tensor(done,device=device)).float().unsqueeze(dim=-1)\n",
    "    advantage=(target-v.detach()).squeeze(dim=-1)\n",
    "    \n",
    "    loss=F.mse_loss(v,target,reduction='sum')-(torch.log(actions[torch.arange(0,action.size(0)),action])@advantage)\n",
    "    loss+=torch.mean(torch.sum(actions*torch.log(actions),dim=-1))\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    #################################################################################\n",
    "    #                                   THE END                                     #\n",
    "    #################################################################################\n",
    "    if t % 1000 == 999:\n",
    "        print('iteration {:5d}: average reward = {:5f}'.format(t + 1, np.mean([test_model(model) for _ in range(10)])))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW6_actor_critic.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
